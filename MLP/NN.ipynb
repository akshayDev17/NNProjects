{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce045c90-3774-45cd-9961-af37b9ef754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39db43-08af-48e9-8928-35fbfdd8eba4",
   "metadata": {},
   "source": [
    "### Define signature of a generic activation function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbacb7f1-367d-4696-b991-02a1ff3f194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class ActivationFunction(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def vectorized_function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def derivative(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def __call__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae26c33-fffa-419a-8b69-49ba10116019",
   "metadata": {},
   "source": [
    "### Define all activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b35986c-5a36-4535-8a90-c8e210ebf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -700, 700)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -100, 100)\n",
    "        # print(x)\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return (expr1-expr2)/(expr1+expr2)\n",
    "        \n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return 4/((expr1+expr2)**2)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.derivative(x)\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0,x) == 0:\n",
    "            return 0\n",
    "        return 1\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0.01*x, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0.01*x, x) == x:\n",
    "            return 1\n",
    "        return 0.01\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class Linear:\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68a466-0b6c-418a-99f9-7c1e8ae56e08",
   "metadata": {},
   "source": [
    "### Import existing loss functions and code new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f45c19-05de-465b-b4b9-fedabcd235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberLoss(y_true, y_pred, delta=10):\n",
    "    err = y_true - y_pred\n",
    "    n_samples = y_true.shape[0]\n",
    "    abs_err = np.abs(err)\n",
    "    delta_sq = 0.5*(delta ** 2)\n",
    "    huber_loss_vectorized = np.vectorize(lambda x: (x**2)*0.5 if x <= delta else delta*x - delta)\n",
    "    huber_loss_vec = huber_loss_vectorized(abs_err)\n",
    "    return np.sum(huber_loss_vec)/n_samples\n",
    "    # return np.sum(huber_loss_vectorized(abs_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ffaaed-5437-49a6-9ea3-29cf1072ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayprabhakant/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss\n",
    "\n",
    "class Crossentropy:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return log_loss(y_true, y_pred, labels = np.arange(y_pred.shape[1]))\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        # return an n x 1 matrix\n",
    "        epsilon = 1e-6\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_pred_capped = np.clip(y_pred, epsilon, 1-epsilon) # cap predicted probabilities to avoid floating point issues after taking reciprocal\n",
    "        y_pred_inv = 1/y_pred_capped # 1/y_hat\n",
    "        n_classes = y_pred.shape[1]\n",
    "        y_true_proba = np.eye(n_classes)[y_true] # for ease of computing the derivative, basically a one-hot encoding\n",
    "        derivative_loss_arr = -np.multiply(y_true_proba, y_pred_inv)# -[y/y_hat, (1-y)/(1-y_hat)]\n",
    "        derivative_loss_arr = np.sum(derivative_loss_arr, axis=1) # sum up, i.e., y/y_hat + (1-y)/(1-y_hat)  \n",
    "        return derivative_loss_arr/n_samples\n",
    "\n",
    "class MSE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return -2*(y_true_reshaped - y_pred)/n_samples\n",
    "\n",
    "class MAE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return np.where(y_true > y_pred, 1, -1)/n_samples\n",
    "\n",
    "class HuberLoss:\n",
    "    def __init__(self, delta=10):\n",
    "        self.delta=delta\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return huberLoss(y_true, y_pred, self.delta)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        err = y_true_reshaped - y_pred\n",
    "        huber_loss_derivative_vectorized = np.vectorize(lambda x: x if np.abs(x) <= self.delta else -self.delta if x < 0 else self.delta)\n",
    "        return huber_loss_derivative_vectorized(err)/n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7b780-94bf-440b-960f-786e48f02727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8646a9d-c285-4312-9f7f-b51d062c1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = np.random.randint(0, 2, size=(3,)), np.random.uniform(0,1,(3,1))\n",
    "print(y_true,\"\\n\\n\", y_pred)\n",
    "y_pred_proba = np.hstack([y_pred, 1-y_pred])\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15309a-e53f-476b-bd70-acc567407175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = Crossentropy()\n",
    "print(f\"Loss = {round(ce(y_true, y_pred_proba), 2)}, Derivative = {ce.derivative(y_true, y_pred_proba)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9bb36-8742-4c46-bf8b-cf83108f2fff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb443d-b218-4d04-9108-bce59294abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_multiclass, y_pred_multiclass_preprocessed = np.random.randint(0, 3, size=(5,)), np.random.uniform(0,1,(5, 3))\n",
    "y_pred_multiclass = y_pred_multiclass_preprocessed / y_pred_multiclass_preprocessed.sum(axis=1, keepdims=True)\n",
    "print(f\"{y_true_multiclass}\\n\\n{y_pred_multiclass}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a39cf-aa0d-4c0b-bf91-cd6f871dc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss = {round(ce(y_true_multiclass, y_pred_multiclass), 2)}, Derivative = {ce.derivative(y_true_multiclass, y_pred_multiclass), 2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50f2b4-75da-4612-aacf-cfffe6d7f47d",
   "metadata": {},
   "source": [
    "Rest assured, the return value is a numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013a013-18a7-43cf-9d0c-d740b36a03a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d43fdc-13ec-499e-9a50-881c2a141b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt, yp = np.random.randint(0,10,(4,)), np.random.randint(0,10,(4,))\n",
    "print(f\"{yt}\\n\\n{yp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b88c25-38c6-4546-8524-7516a0323478",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_fn = MSE()\n",
    "print(f\"Loss = {mse_loss_fn(yt, yp)}\\nderivative = \\n{mse_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64eb971-49c3-4113-97e9-fdf24875b32b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42f94-9aac-4843-992a-32b41fc26515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_loss_fn = MAE()\n",
    "print(f\"Loss = {mae_loss_fn(yt, yp)}\\nderivative = \\n{mae_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f63389-6edc-4423-b5a8-afbabf20b9ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ca8e-1805-423a-b525-2b2a650e28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss_fn = HuberLoss(delta=8)\n",
    "print(f\"Delta = {huber_loss_fn.delta}, Loss = {huber_loss_fn(yt, yp)}\\nderivative = \\n{huber_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93c7d-341c-47fe-9907-d772833d6f01",
   "metadata": {},
   "source": [
    "### Define Layer and Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9443c405-b87a-42df-b3fc-1b07aebf5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListHandler(logging.Handler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log = []\n",
    "\n",
    "    def emit(self, record):\n",
    "        self.log.append(self.format(record))\n",
    "\n",
    "class Layer():\n",
    "    \n",
    "    __valid_activations = {'sigmoid': Sigmoid, 'tanh': Tanh, 'relu': ReLU, 'leaky_relu': LeakyReLU, 'linear': Linear}\n",
    "    \n",
    "    def __init__(self, activation, in_dim, out_dim, learning_rate=0.01):\n",
    "        if activation.lower() not in list(Layer.__valid_activations.keys()):\n",
    "            raise Exception(f\"Valid activations are {Layer.__valid_activations}.\")\n",
    "        self.activation = Layer.__valid_activations[activation.lower()]()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "\n",
    "        logger_name = f'MyClass_{id(self)}'  # Generate a unique name for the logger\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.log_handler = ListHandler()\n",
    "        self.log_handler.setLevel(logging.DEBUG)\n",
    "        self.logger.addHandler(self.log_handler)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - \\n%(message)s')\n",
    "        self.log_handler.setFormatter(formatter)\n",
    "\n",
    "    def forward_compute(self, X, compute_gradient=False):\n",
    "        output_prime = X.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "\n",
    "        if compute_gradient:\n",
    "            # computing stuff for eventual backpropagation\n",
    "            self._activation_gradient = self.activation.vectorized_derivative(output_prime) # of order n x out_dim\n",
    "            self._input = X\n",
    "        \n",
    "        return output_val\n",
    "\n",
    "    def backprop_compute(self, prev_grad_multipliers):\n",
    "        gradient_mat = 1\n",
    "        \n",
    "        # To Do: find a way of multiplying pre v_grad_multipliers with this layer's gradient multiplier matrix.\n",
    "        '''\n",
    "        Needs: \n",
    "         1. current-layer's activation gradient matrix(as a function of this layer's input, i.e. prev layers output)\n",
    "         2. current-layer's input matrix\n",
    "        '''\n",
    "        self.learning_rate /= 2\n",
    "        activ_prev_layer_output_element_wise_product = np.multiply(prev_grad_multipliers, self._activation_gradient) # of order n x out_dim\n",
    "        weights_gradient = self._input.T.dot(activ_prev_layer_output_element_wise_product)\n",
    "        self.weights -= self.learning_rate * weights_gradient\n",
    "        self.logger.info(f\"\\tWeight Gradient = \\n\\t{self.learning_rate * weights_gradient}\\n\")\n",
    "        self.logger.info(f\"\\tWeights = \\n\\t{self.weights}\\n\\n\")\n",
    "\n",
    "        send_mat_to_prev_layer = activ_prev_layer_output_element_wise_product.dot(self.weights.T) # of order n x in_dim, \n",
    "                                                                      # which is basically n x out_dim for the previous layer.\n",
    "        return send_mat_to_prev_layer\n",
    "\n",
    "    def print_logs(self):\n",
    "        for log_record in self.log_handler.log:\n",
    "            print(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb109d2-fe53-409a-8c4c-42da00d6e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSequential():\n",
    "    __valid_loss_functions = {\n",
    "        'crossentropy': Crossentropy, \n",
    "        'mse': MSE,\n",
    "        'mae': MAE,\n",
    "        'huber': HuberLoss\n",
    "    }\n",
    "\n",
    "    # def __init__(self, layers_arr: List, metrics_to_track, early_stopping=False):\n",
    "    def __init__(self, layers_arr: List[Layer]):\n",
    "        self._layers = layers_arr\n",
    "\n",
    "    def compile(self, loss_function, n_iter = 100):\n",
    "        if loss_function.lower() not in ModelSequential.__valid_loss_functions:\n",
    "            raise Exception(f\"Loss functions should be any of {ModelSequential.__valid_loss_functions.keys()}.\")\n",
    "        \n",
    "        self.loss_function = ModelSequential.__valid_loss_functions[loss_function.lower()]()\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def reweight_layers(self):\n",
    "        n_layers = len(self._layers)\n",
    "        for i in range(n_layers):\n",
    "            in_dim, out_dim = self._layers[i].in_dim, self._layers[i].out_dim\n",
    "            self._layers[i].weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "            print(f\"Weights for layer {i} set to \\n{self._layers[i].weights}\\n\\n\")\n",
    "    def fit(self, X, y):\n",
    "        pbar_iterations = tqdm(self.n_iter)\n",
    "        pbar_iterations.set_description(\"#Iterations: \")\n",
    "        for iter_ in range(self.n_iter):\n",
    "            n_layers = len(self._layers)\n",
    "            input_for_next_layer = X.copy()\n",
    "\n",
    "\n",
    "            # forward compute\n",
    "            for i in range(n_layers):\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "\n",
    "            y_pred = self.predict(X)\n",
    "            input_for_next_layer = self.loss_function.derivative(y_true=y, y_pred=y_pred)\n",
    "\n",
    "        \n",
    "            # backprop\n",
    "            for i in range(n_layers-1, -1, -1):\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"error\")\n",
    "                    try:\n",
    "                        input_for_next_layer = self._layers[i].backprop_compute(input_for_next_layer)\n",
    "                    except ValueError as ve:\n",
    "                        return Exception(f\"Backpropagation failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                    except RuntimeWarning as rw:\n",
    "                        raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "\n",
    "            pbar_iterations.update(1)\n",
    "        pbar_iterations.close()\n",
    "\n",
    "    def print_layer_logs(self):\n",
    "        for i in range(n_layers):\n",
    "            print(f\"Handling layer {i}...\")\n",
    "            self._layers[i].print_logs()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_layers = len(self._layers)\n",
    "        input_for_next_layer = X.copy()\n",
    "\n",
    "        # forward compute\n",
    "        for i in range(n_layers):\n",
    "            with warnings.catch_warnings():\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "        return input_for_next_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b5658-359e-446d-98d0-255e327c8e5e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93386b8-4fbb-4913-be5c-56bc27c784c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240120 2400120 24000120 240000120\n",
      "\n",
      "80104 80104 8000104 240000120\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_y(x):\n",
    "    return x[:,0] + (x[:,1]**2) + np.abs(x[:, 2])\n",
    "\n",
    "x_10K = np.random.randint(-100,100,size=(10000, 3))\n",
    "y_10K = get_y(x_10K)\n",
    "\n",
    "# Column-wise normalization using L2 normalization (Euclidean norm)\n",
    "x_10K_normalized = x_10K / np.linalg.norm(x_10K, axis=0)  # Normalizing along axis 0 (columns)\n",
    "\n",
    "x_100K = np.random.randint(-100,100,size=(100000, 3))\n",
    "y_100K = get_y(x_10K)\n",
    "\n",
    "x_1M = np.random.randint(-100,100,size=(1000000, 3))\n",
    "y_1M = get_y(x_1M)\n",
    "\n",
    "x_10M = np.random.randint(-100,100,size=(10000000, 3))\n",
    "y_10M = get_y(x_10M)\n",
    "\n",
    "print(sys.getsizeof(x_10K), sys.getsizeof(x_100K), sys.getsizeof(x_1M), sys.getsizeof(x_10M))\n",
    "print()\n",
    "print(sys.getsizeof(y_10K), sys.getsizeof(y_100K), sys.getsizeof(y_1M), sys.getsizeof(x_10M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f42f9b1-2396-4737-b445-fe16dd166250",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, l3 = Layer('sigmoid', x_10K.shape[1], 3, learning_rate=0.01), Layer('leaky_relu', 3, 2, learning_rate=0.01), Layer('linear', 2, 1, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48764d6d-78d3-4409-8cad-b8be8994f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSequential([l1, l2, l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d70ef83-9928-4d0a-8367-744dee21e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss_function='huber', n_iter=10)\n",
    "model.compile(loss_function='mse', n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff69c889-7e7e-40f9-aa43-190b52711599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181473.281987637\n"
     ]
    }
   ],
   "source": [
    "init_pred_10k = model.predict(x_10K_normalized)\n",
    "print(model.loss_function(y_true=y_10K, y_pred=init_pred_10k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f01b1bad-4fec-4c46-ba0b-783b183e4199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "[[ 0.83362112  0.92637197  0.23902398]\n",
      " [ 0.21962276 -0.66480166 -0.76965921]\n",
      " [-0.87562595 -0.88478715 -0.03543006]]\n",
      "\n",
      "\n",
      "Layer 1\n",
      "[[-0.14251452  0.83318855]\n",
      " [-0.60185958  0.85707514]\n",
      " [ 0.58464671  0.92996537]]\n",
      "\n",
      "\n",
      "Layer 2\n",
      "[[0.84554119]\n",
      " [0.15461206]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009358882904052734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034f273d13a047b0a92ce4d27dba241a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.reweight_layers()\n",
    "\n",
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")\n",
    "\n",
    "\n",
    "model.fit(x_10K_normalized, y_10K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de1cad46-23c5-4c45-b5b4-b16512d5caf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling layer 0...\n",
      "2023-11-24 13:54:03,256 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-46.34617646 -46.3503554  -46.35194443]\n",
      " [ 24.3774111   24.37423523  24.3758658 ]\n",
      " [ 16.44985882  16.45107686  16.45310844]]\n",
      "\n",
      "2023-11-24 13:54:03,256 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[ 47.17979758  47.27672737  46.59096841]\n",
      " [-24.15778834 -25.0390369  -25.14552501]\n",
      " [-17.32548477 -17.33586401 -16.48853849]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:04,357 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-1.06480716e+12 -1.06317238e+12 -1.06633856e+12]\n",
      " [ 4.95363521e+11  4.92117551e+11  4.93035422e+11]\n",
      " [ 3.77533049e+11  3.77101899e+11  3.80432617e+11]]\n",
      "\n",
      "2023-11-24 13:54:04,357 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[ 1.06480716e+12  1.06317238e+12  1.06633856e+12]\n",
      " [-4.95363521e+11 -4.92117551e+11 -4.93035422e+11]\n",
      " [-3.77533049e+11 -3.77101899e+11 -3.80432617e+11]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:05,471 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-5.70383432e-252  1.14091698e-252 -1.35570294e-252]\n",
      " [-1.16434065e-251  9.10747353e-253  2.46013305e-252]\n",
      " [-8.62664540e-253  2.00611493e-252 -7.02739147e-252]]\n",
      "\n",
      "2023-11-24 13:54:05,471 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[ 1.06480716e+12  1.06317238e+12  1.06633856e+12]\n",
      " [-4.95363521e+11 -4.92117551e+11 -4.93035422e+11]\n",
      " [-3.77533049e+11 -3.77101899e+11 -3.80432617e+11]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:06,571 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-2.51948053e-71  5.03893620e-72 -5.98552711e-72]\n",
      " [-5.14383541e-71  4.01575350e-72  1.08687133e-71]\n",
      " [-3.80080473e-72  8.86877431e-72 -3.10356218e-71]]\n",
      "\n",
      "2023-11-24 13:54:06,571 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[ 1.06480716e+12  1.06317238e+12  1.06633856e+12]\n",
      " [-4.95363521e+11 -4.92117551e+11 -4.93035422e+11]\n",
      " [-3.77533049e+11 -3.77101899e+11 -3.80432617e+11]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:07,671 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "2023-11-24 13:54:07,671 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:08,779 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "2023-11-24 13:54:08,779 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:09,876 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "2023-11-24 13:54:09,876 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:10,974 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "2023-11-24 13:54:10,974 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:12,080 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "2023-11-24 13:54:12,080 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:13,196 - MyClass_5473425728 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "2023-11-24 13:54:13,196 - MyClass_5473425728 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "\n",
      "\n",
      "Handling layer 1...\n",
      "2023-11-24 13:54:03,256 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-1.37742411e-01 -7.44374001e+02]\n",
      " [-1.37748869e-01 -7.44408901e+02]\n",
      " [-1.37738124e-01 -7.44350831e+02]]\n",
      "\n",
      "2023-11-24 13:54:03,256 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[-4.77210638e-03  7.45207189e+02]\n",
      " [-4.64110715e-01  7.45265976e+02]\n",
      " [ 7.22384839e-01  7.45280796e+02]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:04,357 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-3.73780013e+03 -3.39177983e+07]\n",
      " [-3.73956120e+03 -3.39337787e+07]\n",
      " [-3.73631666e+03 -3.39043369e+07]]\n",
      "\n",
      "2023-11-24 13:54:04,357 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[3.73779536e+03 3.39185435e+07]\n",
      " [3.73909709e+03 3.39345239e+07]\n",
      " [3.73703905e+03 3.39050822e+07]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:05,471 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-1.37782463e+25 -1.25027414e+29]\n",
      " [-1.37810009e+25 -1.25052409e+29]\n",
      " [-1.37800806e+25 -1.25044059e+29]]\n",
      "\n",
      "2023-11-24 13:54:05,471 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[1.37782463e+25 1.25027414e+29]\n",
      " [1.37810009e+25 1.25052409e+29]\n",
      " [1.37800806e+25 1.25044059e+29]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:06,570 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-2.89587956e+115 -2.62779691e+119]\n",
      " [-2.89645845e+115 -2.62832222e+119]\n",
      " [-2.89626552e+115 -2.62814714e+119]]\n",
      "\n",
      "2023-11-24 13:54:06,571 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[2.89587956e+115 2.62779691e+119]\n",
      " [2.89645845e+115 2.62832222e+119]\n",
      " [2.89626552e+115 2.62814714e+119]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:07,670 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "2023-11-24 13:54:07,670 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:08,778 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "2023-11-24 13:54:08,778 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:09,876 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "2023-11-24 13:54:09,876 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:10,974 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "2023-11-24 13:54:10,974 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:12,080 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "2023-11-24 13:54:12,080 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:13,196 - MyClass_5473425536 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "2023-11-24 13:54:13,196 - MyClass_5473425536 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      "\n",
      "\n",
      "Handling layer 2...\n",
      "2023-11-24 13:54:03,255 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[ 2.68825463e-02]\n",
      " [-4.40865336e+01]]\n",
      "\n",
      "2023-11-24 13:54:03,255 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[ 0.81865864]\n",
      " [44.24114562]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:04,357 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[3.11634910e+01]\n",
      " [2.75401367e+05]]\n",
      "\n",
      "2023-11-24 13:54:04,357 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[-3.03448323e+01]\n",
      " [-2.75357126e+05]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:05,471 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-3.93097272e+14]\n",
      " [-3.56706754e+18]]\n",
      "\n",
      "2023-11-24 13:54:05,471 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[3.93097272e+14]\n",
      " [3.56706754e+18]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:06,570 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[3.46015911e+70]\n",
      " [3.13983895e+74]]\n",
      "\n",
      "2023-11-24 13:54:06,570 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[-3.46015911e+70]\n",
      " [-3.13983895e+74]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:07,670 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[-inf]\n",
      " [-inf]]\n",
      "\n",
      "2023-11-24 13:54:07,670 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[inf]\n",
      " [inf]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:08,778 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "2023-11-24 13:54:08,778 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:09,876 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "2023-11-24 13:54:09,876 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:10,974 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "2023-11-24 13:54:10,974 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:12,079 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "2023-11-24 13:54:12,079 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "\n",
      "2023-11-24 13:54:13,196 - MyClass_5477244496 - INFO - \n",
      "\tWeight Gradient = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "2023-11-24 13:54:13,196 - MyClass_5477244496 - INFO - \n",
      "\tWeights = \n",
      "\t[[nan]\n",
      " [nan]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.print_layer_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2861423-e675-497b-a82c-f7616166367f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m y_pred_10k \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_10K_normalized)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_10K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_10k\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mMSE.__call__\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mMSE.function\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:101\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    104\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "y_pred_10k = model.predict(x_10K_normalized)\n",
    "model.loss_function(y_true=y_10K, y_pred=y_pred_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e39e7-4935-457e-9bf0-c1e5e866329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e42df-a66f-408b-b865-25240a123d34",
   "metadata": {},
   "source": [
    "### Print all layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851fd04-e1aa-4622-8351-18e40e8a244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0143f7-f2a9-49c5-9933-a2156aece7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86345908-fcbf-4e19-8dd3-c58e115cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7893e-4064-401c-94c2-b4f4017a73b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
