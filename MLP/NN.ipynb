{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce045c90-3774-45cd-9961-af37b9ef754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39db43-08af-48e9-8928-35fbfdd8eba4",
   "metadata": {},
   "source": [
    "### Define signature of a generic activation function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbacb7f1-367d-4696-b991-02a1ff3f194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class ActivationFunction(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def vectorized_function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def derivative(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def __call__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae26c33-fffa-419a-8b69-49ba10116019",
   "metadata": {},
   "source": [
    "### Define all activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b35986c-5a36-4535-8a90-c8e210ebf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -700, 700)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -100, 100)\n",
    "        # print(x)\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return (expr1-expr2)/(expr1+expr2)\n",
    "        \n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return 4/((expr1+expr2)**2)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.derivative(x)\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0,x) == 0:\n",
    "            return 0\n",
    "        return 1\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0.01*x, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0.01*x, x) == x:\n",
    "            return 1\n",
    "        return 0.01\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class Linear:\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68a466-0b6c-418a-99f9-7c1e8ae56e08",
   "metadata": {},
   "source": [
    "### Import existing loss functions and code new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f45c19-05de-465b-b4b9-fedabcd235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberLoss(y_true, y_pred, delta=10):\n",
    "    err = y_true - y_pred\n",
    "    abs_err = np.abs(err)\n",
    "    delta_sq = 0.5*(delta ** 2)\n",
    "    huber_loss_vectorized = np.vectorize(lambda x: (x**2)*0.5 if x <= delta else delta*x - delta)\n",
    "    huber_loss_vec = huber_loss_vectorized(abs_err)\n",
    "    return np.sum(huber_loss_vec)\n",
    "    # return np.sum(huber_loss_vectorized(abs_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c8ffaaed-5437-49a6-9ea3-29cf1072ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss\n",
    "\n",
    "class Crossentropy:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return log_loss(y_true, y_pred, labels = np.arange(y_pred.shape[1]))\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        # return an n x 1 matrix\n",
    "        epsilon = 1e-6\n",
    "        y_pred_capped = np.clip(y_pred, epsilon, 1-epsilon) # cap predicted probabilities to avoid floating point issues after taking reciprocal\n",
    "        y_pred_inv = 1/y_pred_capped # 1/y_hat\n",
    "        n_classes = y_pred.shape[1]\n",
    "        y_true_proba = np.eye(n_classes)[y_true] # for ease of computing the derivative, basically a one-hot encoding\n",
    "        derivative_loss_arr = -np.multiply(y_true_proba, y_pred_inv)# -[y/y_hat, (1-y)/(1-y_hat)]\n",
    "        derivative_loss_arr = np.sum(derivative_loss_arr, axis=1) # sum up, i.e., y/y_hat + (1-y)/(1-y_hat)  \n",
    "        return derivative_loss_arr\n",
    "\n",
    "class MSE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return -2*(y_true_reshaped - y_pred)\n",
    "\n",
    "class MAE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return np.where(y_true > y_pred, 1, -1)\n",
    "\n",
    "class HuberLoss:\n",
    "    def __init__(self, delta=10):\n",
    "        self.delta=delta\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return huberLoss(y_true, y_pred, self.delta)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        err = y_true_reshaped - y_pred\n",
    "        huber_loss_derivative_vectorized = np.vectorize(lambda x: x if np.abs(x) <= self.delta else -self.delta if x < 0 else self.delta)\n",
    "        return huber_loss_derivative_vectorized(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7b780-94bf-440b-960f-786e48f02727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8646a9d-c285-4312-9f7f-b51d062c1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = np.random.randint(0, 2, size=(3,)), np.random.uniform(0,1,(3,1))\n",
    "print(y_true,\"\\n\\n\", y_pred)\n",
    "y_pred_proba = np.hstack([y_pred, 1-y_pred])\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15309a-e53f-476b-bd70-acc567407175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = Crossentropy()\n",
    "print(f\"Loss = {round(ce(y_true, y_pred_proba), 2)}, Derivative = {ce.derivative(y_true, y_pred_proba)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9bb36-8742-4c46-bf8b-cf83108f2fff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb443d-b218-4d04-9108-bce59294abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_multiclass, y_pred_multiclass_preprocessed = np.random.randint(0, 3, size=(5,)), np.random.uniform(0,1,(5, 3))\n",
    "y_pred_multiclass = y_pred_multiclass_preprocessed / y_pred_multiclass_preprocessed.sum(axis=1, keepdims=True)\n",
    "print(f\"{y_true_multiclass}\\n\\n{y_pred_multiclass}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a39cf-aa0d-4c0b-bf91-cd6f871dc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss = {round(ce(y_true_multiclass, y_pred_multiclass), 2)}, Derivative = {ce.derivative(y_true_multiclass, y_pred_multiclass), 2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50f2b4-75da-4612-aacf-cfffe6d7f47d",
   "metadata": {},
   "source": [
    "Rest assured, the return value is a numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013a013-18a7-43cf-9d0c-d740b36a03a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d43fdc-13ec-499e-9a50-881c2a141b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt, yp = np.random.randint(0,10,(4,)), np.random.randint(0,10,(4,))\n",
    "print(f\"{yt}\\n\\n{yp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b88c25-38c6-4546-8524-7516a0323478",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_fn = MSE()\n",
    "print(f\"Loss = {mse_loss_fn(yt, yp)}\\nderivative = \\n{mse_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64eb971-49c3-4113-97e9-fdf24875b32b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42f94-9aac-4843-992a-32b41fc26515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_loss_fn = MAE()\n",
    "print(f\"Loss = {mae_loss_fn(yt, yp)}\\nderivative = \\n{mae_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f63389-6edc-4423-b5a8-afbabf20b9ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ca8e-1805-423a-b525-2b2a650e28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss_fn = HuberLoss(delta=8)\n",
    "print(f\"Delta = {huber_loss_fn.delta}, Loss = {huber_loss_fn(yt, yp)}\\nderivative = \\n{huber_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93c7d-341c-47fe-9907-d772833d6f01",
   "metadata": {},
   "source": [
    "### Define Layer and Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9443c405-b87a-42df-b3fc-1b07aebf5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    __valid_activations = {'sigmoid': Sigmoid, 'tanh': Tanh, 'relu': ReLU, 'leaky_relu': LeakyReLU, 'linear': Linear}\n",
    "    \n",
    "    def __init__(self, activation, in_dim, out_dim, learning_rate=0.01):\n",
    "        if activation.lower() not in list(Layer.__valid_activations.keys()):\n",
    "            raise Exception(f\"Valid activations are {Layer.__valid_activations}.\")\n",
    "        self.activation = Layer.__valid_activations[activation.lower()]()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "\n",
    "    def forward_compute(self, X, compute_gradient=False):\n",
    "        output_prime = X.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "\n",
    "        if compute_gradient:\n",
    "            # computing stuff for eventual backpropagation\n",
    "            self._activation_gradient = self.activation.vectorized_derivative(output_prime) # of order n x out_dim\n",
    "            self._input = X\n",
    "        \n",
    "        return output_val\n",
    "\n",
    "    def backprop_compute(self, prev_grad_multipliers, verbose=False):\n",
    "        gradient_mat = 1\n",
    "        \n",
    "        # To Do: find a way of multiplying pre v_grad_multipliers with this layer's gradient multiplier matrix.\n",
    "        '''\n",
    "        Needs: \n",
    "         1. current-layer's activation gradient matrix(as a function of this layer's input, i.e. prev layers output)\n",
    "         2. current-layer's input matrix\n",
    "        '''\n",
    "\n",
    "        activ_prev_layer_output_element_wise_product = np.multiply(prev_grad_multipliers, self._activation_gradient) # of order n x out_dim\n",
    "        weights_gradient = self._input.T.dot(activ_prev_layer_output_element_wise_product)\n",
    "        self.weights -= self.learning_rate * weights_gradient\n",
    "        if verbose:\n",
    "            print(f\"\\tElementwise product of next layer output and this layer's activation gradient = \\n{activ_prev_layer_output_element_wise_product}\\n\\n\")\n",
    "            print(f\"\\tWeight Gradient = \\n{weights_gradient}\\n\\n\")\n",
    "\n",
    "        send_mat_to_prev_layer = activ_prev_layer_output_element_wise_product.dot(self.weights.T) # of order n x in_dim, \n",
    "                                                                      # which is basically n x out_dim for the previous layer.\n",
    "        return send_mat_to_prev_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0eb109d2-fe53-409a-8c4c-42da00d6e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSequential():\n",
    "    __valid_loss_functions = {\n",
    "        'crossentropy': Crossentropy, \n",
    "        'mse': MSE,\n",
    "        'mae': MAE,\n",
    "        'huber': HuberLoss\n",
    "    }\n",
    "\n",
    "    # def __init__(self, layers_arr: List, metrics_to_track, early_stopping=False):\n",
    "    def __init__(self, layers_arr: List[Layer]):\n",
    "        self._layers = layers_arr\n",
    "\n",
    "    def compile(self, loss_function, n_iter = 100):\n",
    "        if loss_function.lower() not in ModelSequential.__valid_loss_functions:\n",
    "            raise Exception(f\"Loss functions should be any of {ModelSequential.__valid_loss_functions.keys()}.\")\n",
    "        \n",
    "        self.loss_function = ModelSequential.__valid_loss_functions[loss_function.lower()]()\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def reweight_layers(self):\n",
    "        n_layers = len(self._layers)\n",
    "        for i in range(n_layers):\n",
    "            in_dim, out_dim = self._layers[i].in_dim, self._layers[i].out_dim\n",
    "            self._layers[i].weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        pbar_iterations = tqdm(self.n_iter)\n",
    "        pbar_iterations.set_description(\"#Iterations: \")\n",
    "        for iter_ in range(self.n_iter):\n",
    "            n_layers = len(self._layers)\n",
    "            input_for_next_layer = X.copy()\n",
    "\n",
    "\n",
    "            # forward compute\n",
    "            for i in range(n_layers):\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "\n",
    "            y_pred = self.predict(X)\n",
    "            input_for_next_layer = self.loss_function.derivative(y_true=y, y_pred=y_pred)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Gradient of loss function has shape {input_for_next_layer.shape} and is = \\n{input_for_next_layer}\\n\\n\")\n",
    "\n",
    "        \n",
    "            # backprop\n",
    "            for i in range(n_layers-1, -1, -1):\n",
    "                if verbose:\n",
    "                    print(f\"Handling layer {i}...\")\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"error\")\n",
    "                    try:\n",
    "                        input_for_next_layer = self._layers[i].backprop_compute(input_for_next_layer, verbose)\n",
    "                    except ValueError as ve:\n",
    "                        return Exception(f\"Backpropagation failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                    except RuntimeWarning:\n",
    "                        raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {ve}\")\n",
    "\n",
    "            pbar_iterations.update(1)\n",
    "        pbar_iterations.close()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_layers = len(self._layers)\n",
    "        input_for_next_layer = X.copy()\n",
    "\n",
    "        # forward compute\n",
    "        for i in range(n_layers):\n",
    "            with warnings.catch_warnings():\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "        return input_for_next_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b5658-359e-446d-98d0-255e327c8e5e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93386b8-4fbb-4913-be5c-56bc27c784c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240120 2400120 24000120 240000120\n",
      "\n",
      "80104 80104 8000104 240000120\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def get_y(x):\n",
    "    return x[:,0] + (x[:,1]**2) + np.abs(x[:, 2])\n",
    "\n",
    "x_10K = np.random.randint(-100,100,size=(10000, 3))\n",
    "y_10K = get_y(x_10K)\n",
    "\n",
    "# Column-wise normalization using L2 normalization (Euclidean norm)\n",
    "x_10K_normalized = x_10K / np.linalg.norm(x_10K, axis=0)  # Normalizing along axis 0 (columns)\n",
    "\n",
    "x_100K = np.random.randint(-100,100,size=(100000, 3))\n",
    "y_100K = get_y(x_10K)\n",
    "\n",
    "x_1M = np.random.randint(-100,100,size=(1000000, 3))\n",
    "y_1M = get_y(x_1M)\n",
    "\n",
    "x_10M = np.random.randint(-100,100,size=(10000000, 3))\n",
    "y_10M = get_y(x_10M)\n",
    "\n",
    "print(sys.getsizeof(x_10K), sys.getsizeof(x_100K), sys.getsizeof(x_1M), sys.getsizeof(x_10M))\n",
    "print()\n",
    "print(sys.getsizeof(y_10K), sys.getsizeof(y_100K), sys.getsizeof(y_1M), sys.getsizeof(x_10M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f42f9b1-2396-4737-b445-fe16dd166250",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, l3 = Layer('sigmoid', x_10K.shape[1], 3), Layer('leaky_relu', 3, 2), Layer('linear', 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48764d6d-78d3-4409-8cad-b8be8994f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSequential([l1, l2, l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6d70ef83-9928-4d0a-8367-744dee21e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss_function='huber', n_iter=10)\n",
    "model.compile(loss_function='mse', n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff69c889-7e7e-40f9-aa43-190b52711599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321433549345.393\n"
     ]
    }
   ],
   "source": [
    "init_pred_10k = model.predict(x_10K_normalized)\n",
    "print(model.loss_function(y_true=y_10K, y_pred=init_pred_10k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "000d6581-6f86-43ac-b050-41db14b6b0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "[[-0.95935186  0.04597412  0.20524535]\n",
      " [-0.55453881 -0.72329169  0.2728556 ]\n",
      " [ 0.86915297  0.06818314 -0.97932693]]\n",
      "\n",
      "\n",
      "Layer 1\n",
      "[[-0.80107311 -0.6941181 ]\n",
      " [ 0.86440511 -0.91224689]\n",
      " [ 0.77003371  0.7647649 ]]\n",
      "\n",
      "\n",
      "Layer 2\n",
      "[[0.8650483 ]\n",
      " [0.79471011]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f01b1bad-4fec-4c46-ba0b-783b183e4199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007968902587890625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761e156eedaa482792c3d948e63373af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss function has shape (10000, 1) and is = \n",
      "[[ -320.00871465]\n",
      " [-1746.00870167]\n",
      " [-4800.00872172]\n",
      " ...\n",
      " [-7846.00869045]\n",
      " [-2682.0087321 ]\n",
      " [-1988.00870162]]\n",
      "\n",
      "\n",
      "Handling layer 2...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[ -320.00871465]\n",
      " [-1746.00870167]\n",
      " [-4800.00872172]\n",
      " ...\n",
      " [-7846.00869045]\n",
      " [-2682.0087321 ]\n",
      " [-1988.00870162]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[358560.88925969]\n",
      " [348545.51446022]]\n",
      "\n",
      "\n",
      "Handling layer 1...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[ 11472.942497    11152.46298015]\n",
      " [ 62597.84973414  60849.27227515]\n",
      " [172089.76358354 167282.6930084 ]\n",
      " ...\n",
      " [281294.85984139 273437.30797123]\n",
      " [ 96155.29374922  93469.33920085]\n",
      " [ 71274.0262148   69283.0927214 ]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[1.19072453e+09 1.15746342e+09]\n",
      " [1.19069722e+09 1.15743687e+09]\n",
      " [1.19112935e+09 1.15785693e+09]]\n",
      "\n",
      "\n",
      "Handling layer 0...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[-6.64240902e+10 -6.64221314e+10 -6.64467910e+10]\n",
      " [-3.62409757e+11 -3.62409321e+11 -3.62532198e+11]\n",
      " [-9.96325844e+11 -9.96278365e+11 -9.96645878e+11]\n",
      " ...\n",
      " [-1.62852711e+12 -1.62854319e+12 -1.62912060e+12]\n",
      " [-5.56702394e+11 -5.56668261e+11 -5.56875958e+11]\n",
      " [-4.12630936e+11 -4.12640233e+11 -4.12770648e+11]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[4.09700621e+11 4.09391876e+11 4.09675213e+11]\n",
      " [5.42963388e+12 5.42931961e+12 5.43142585e+12]\n",
      " [2.09461301e+12 2.09455371e+12 2.09523633e+12]]\n",
      "\n",
      "\n",
      "Gradient of loss function has shape (10000, 1) and is = \n",
      "[[ 4.98220599e+09]\n",
      " [ 4.98220457e+09]\n",
      " [ 4.98220151e+09]\n",
      " ...\n",
      " [ 4.98219847e+09]\n",
      " [ 4.98220363e+09]\n",
      " [-1.98800000e+03]]\n",
      "\n",
      "\n",
      "Handling layer 2...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[ 4.98220599e+09]\n",
      " [ 4.98220457e+09]\n",
      " [ 4.98220151e+09]\n",
      " ...\n",
      " [ 4.98219847e+09]\n",
      " [ 4.98220363e+09]\n",
      " [-1.98800000e+03]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[-8.99569704e+18]\n",
      " [-8.74441567e+18]]\n",
      "\n",
      "\n",
      "Handling layer 1...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[ 4.48184157e+24  4.35664802e+24]\n",
      " [ 4.48184028e+24  4.35664677e+24]\n",
      " [ 4.48183754e+24  4.35664410e+24]\n",
      " ...\n",
      " [ 4.48183480e+24  4.35664143e+24]\n",
      " [ 4.48183944e+24  4.35664595e+24]\n",
      " [-1.78834457e+18 -1.73838984e+18]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[2.26511982e+28 2.20184708e+28]\n",
      " [2.26511982e+28 2.20184708e+28]\n",
      " [2.26511982e+28 2.20184708e+28]]\n",
      "\n",
      "\n",
      "Handling layer 0...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[-0.0000000e+000 -0.0000000e+000 -0.0000000e+000]\n",
      " [-0.0000000e+000 -0.0000000e+000 -0.0000000e+000]\n",
      " [-0.0000000e+000 -0.0000000e+000 -0.0000000e+000]\n",
      " ...\n",
      " [-0.0000000e+000 -0.0000000e+000 -0.0000000e+000]\n",
      " [-0.0000000e+000 -0.0000000e+000 -0.0000000e+000]\n",
      " [ 7.7679297e-260  7.7679297e-260  7.7679297e-260]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[7.39752859e-260 7.39752859e-260 7.39752859e-260]\n",
      " [1.62435694e-257 1.62435694e-257 1.62435694e-257]\n",
      " [3.55989904e-259 3.55989904e-259 3.55989904e-259]]\n",
      "\n",
      "\n",
      "Gradient of loss function has shape (10000, 1) and is = \n",
      "[[-2.37781186e+42]\n",
      " [-2.37781186e+42]\n",
      " [-2.37781186e+42]\n",
      " ...\n",
      " [-2.37781186e+42]\n",
      " [-2.37781186e+42]\n",
      " [-1.98800000e+03]]\n",
      "\n",
      "\n",
      "Handling layer 2...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[-2.37781186e+42]\n",
      " [-2.37781186e+42]\n",
      " [-2.37781186e+42]\n",
      " ...\n",
      " [-2.37781186e+42]\n",
      " [-2.37781186e+42]\n",
      " [-1.98800000e+03]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[8.16629682e+70]\n",
      " [7.93818351e+70]]\n",
      "\n",
      "\n",
      "Handling layer 1...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[1.94179174e+109 1.88755069e+109]\n",
      " [1.94179174e+109 1.88755069e+109]\n",
      " [1.94179174e+109 1.88755069e+109]\n",
      " ...\n",
      " [1.94179174e+109 1.88755069e+109]\n",
      " [1.94179174e+109 1.88755069e+109]\n",
      " [1.62345981e+070 1.57811088e+070]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[9.81381548e+112 9.53968120e+112]\n",
      " [9.81381548e+112 9.53968120e+112]\n",
      " [9.81381548e+112 9.53968120e+112]]\n",
      "\n",
      "\n",
      "Handling layer 0...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[-0.00000000e+000 -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000]\n",
      " ...\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000]\n",
      " [-0.00000000e+000 -0.00000000e+000 -0.00000000e+000]\n",
      " [-3.05521893e-123 -3.05521893e-123 -3.05521893e-123]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[-2.90953577e-123 -2.90953577e-123 -2.90953577e-123]\n",
      " [-6.38878860e-121 -6.38878860e-121 -6.38878860e-121]\n",
      " [-1.40015053e-122 -1.40015053e-122 -1.40015053e-122]]\n",
      "\n",
      "\n",
      "Gradient of loss function has shape (10000, 1) and is = \n",
      "[[ 9.35221621e+178]\n",
      " [ 9.35221621e+178]\n",
      " [ 9.35221621e+178]\n",
      " ...\n",
      " [ 9.35221621e+178]\n",
      " [ 9.35221621e+178]\n",
      " [-1.98800000e+003]]\n",
      "\n",
      "\n",
      "Handling layer 2...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[ 9.35221621e+178]\n",
      " [ 9.35221621e+178]\n",
      " [ 9.35221621e+178]\n",
      " ...\n",
      " [ 9.35221621e+178]\n",
      " [ 9.35221621e+178]\n",
      " [-1.98800000e+003]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[-1.39158237e+292]\n",
      " [-1.35271060e+292]]\n",
      "\n",
      "\n",
      "Handling layer 1...\n",
      "\tElementwise product of next layer output and this layer's activation gradient = \n",
      "[[             inf              inf]\n",
      " [             inf              inf]\n",
      " [             inf              inf]\n",
      " ...\n",
      " [             inf              inf]\n",
      " [             inf              inf]\n",
      " [-2.76646576e+291 -2.68918867e+291]]\n",
      "\n",
      "\n",
      "\tWeight Gradient = \n",
      "[[inf inf]\n",
      " [inf inf]\n",
      " [inf inf]]\n",
      "\n",
      "\n",
      "Handling layer 0...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 've' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 56\u001b[0m, in \u001b[0;36mModelSequential.fit\u001b[0;34m(self, X, y, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     input_for_next_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_for_next_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n",
      "Cell \u001b[0;32mIn[94], line 35\u001b[0m, in \u001b[0;36mLayer.backprop_compute\u001b[0;34m(self, prev_grad_multipliers, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mNeeds: \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m 1. current-layer's activation gradient matrix(as a function of this layer's input, i.e. prev layers output)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m 2. current-layer's input matrix\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m activ_prev_layer_output_element_wise_product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_grad_multipliers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activation_gradient\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# of order n x out_dim\u001b[39;00m\n\u001b[1;32m     36\u001b[0m weights_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(activ_prev_layer_output_element_wise_product)\n",
      "\u001b[0;31mRuntimeWarning\u001b[0m: invalid value encountered in multiply",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mreweight_layers()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_10K_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_10K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[95], line 60\u001b[0m, in \u001b[0;36mModelSequential.fit\u001b[0;34m(self, X, y, verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackpropagation failed at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the following exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward compute failed at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the following exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mve\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     pbar_iterations\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m pbar_iterations\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 've' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model.reweight_layers()\n",
    "model.fit(x_10K_normalized, y_10K, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2861423-e675-497b-a82c-f7616166367f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.901231073506451e+123"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_10k = model.predict(x_10K_normalized)\n",
    "model.loss_function(y_true=y_10K, y_pred=y_pred_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e42df-a66f-408b-b865-25240a123d34",
   "metadata": {},
   "source": [
    "### Print all layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1851fd04-e1aa-4622-8351-18e40e8a244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "[[-4.09700621e+09 -4.09391876e+09 -4.09675213e+09]\n",
      " [-5.42963388e+10 -5.42931961e+10 -5.43142585e+10]\n",
      " [-2.09461301e+10 -2.09455371e+10 -2.09523633e+10]]\n",
      "\n",
      "\n",
      "Layer 1\n",
      "[[-inf -inf]\n",
      " [-inf -inf]\n",
      " [-inf -inf]]\n",
      "\n",
      "\n",
      "Layer 2\n",
      "[[1.39158237e+290]\n",
      " [1.35271060e+290]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b0143f7-f2a9-49c5-9933-a2156aece7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_ModelSequential__layers', '_ModelSequential__valid_loss_functions', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'compile', 'fit', 'loss_function', 'n_iter', 'predict']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86345908-fcbf-4e19-8dd3-c58e115cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f523e94-6b3c-4c70-8114-908ded721370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
