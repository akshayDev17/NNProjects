{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce045c90-3774-45cd-9961-af37b9ef754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "from typing import List\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39db43-08af-48e9-8928-35fbfdd8eba4",
   "metadata": {},
   "source": [
    "### Define signature of a generic activation function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbacb7f1-367d-4696-b991-02a1ff3f194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class ActivationFunction(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def vectorized_function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def derivative(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def __call__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae26c33-fffa-419a-8b69-49ba10116019",
   "metadata": {},
   "source": [
    "### Define all activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b35986c-5a36-4535-8a90-c8e210ebf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -700, 700)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -100, 100)\n",
    "        # print(x)\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return (expr1-expr2)/(expr1+expr2)\n",
    "        \n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return 4/((expr1+expr2)**2)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.derivative(x)\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0,x) == 0:\n",
    "            return 0\n",
    "        return 1\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0.3*x, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0.3*x, x) == x:\n",
    "            return 1\n",
    "        return 0.3\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class Linear:\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68a466-0b6c-418a-99f9-7c1e8ae56e08",
   "metadata": {},
   "source": [
    "### Import existing loss functions and code new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f45c19-05de-465b-b4b9-fedabcd235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberLoss(y_true, y_pred, delta=10):\n",
    "    err = y_true - y_pred\n",
    "    n_samples = y_true.shape[0]\n",
    "    abs_err = np.abs(err)\n",
    "    delta_sq = 0.5*(delta ** 2)\n",
    "    huber_loss_vectorized = np.vectorize(lambda x: (x**2)*0.5 if x <= delta else delta*x - delta)\n",
    "    huber_loss_vec = huber_loss_vectorized(abs_err)\n",
    "    return np.sum(huber_loss_vec)/n_samples\n",
    "    # return np.sum(huber_loss_vectorized(abs_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ffaaed-5437-49a6-9ea3-29cf1072ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshayprabhakant/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss\n",
    "\n",
    "class Crossentropy:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return log_loss(y_true, y_pred, labels = np.arange(y_pred.shape[1]))\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        # return an n x 1 matrix\n",
    "        epsilon = 1e-6\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_pred_capped = np.clip(y_pred, epsilon, 1-epsilon) # cap predicted probabilities to avoid floating point issues after taking reciprocal\n",
    "        y_pred_inv = 1/y_pred_capped # 1/y_hat\n",
    "        n_classes = y_pred.shape[1]\n",
    "        y_true_proba = np.eye(n_classes)[y_true] # for ease of computing the derivative, basically a one-hot encoding\n",
    "        derivative_loss_arr = -np.multiply(y_true_proba, y_pred_inv)# -[y/y_hat, (1-y)/(1-y_hat)]\n",
    "        derivative_loss_arr = np.sum(derivative_loss_arr, axis=1) # sum up, i.e., y/y_hat + (1-y)/(1-y_hat)  \n",
    "        return derivative_loss_arr/n_samples\n",
    "\n",
    "class MSE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return (-2*(y_true_reshaped - y_pred))/n_samples\n",
    "\n",
    "class MAE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return np.where(y_true > y_pred, 1, -1)/n_samples\n",
    "\n",
    "class HuberLoss:\n",
    "    def __init__(self, delta=10):\n",
    "        self.delta=delta\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return huberLoss(y_true, y_pred, self.delta)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        err = y_true_reshaped - y_pred\n",
    "        huber_loss_derivative_vectorized = np.vectorize(lambda x: x if np.abs(x) <= self.delta else -self.delta if x < 0 else self.delta)\n",
    "        return huber_loss_derivative_vectorized(err)/n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7b780-94bf-440b-960f-786e48f02727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8646a9d-c285-4312-9f7f-b51d062c1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = np.random.randint(0, 2, size=(3,)), np.random.uniform(0,1,(3,1))\n",
    "print(y_true,\"\\n\\n\", y_pred)\n",
    "y_pred_proba = np.hstack([y_pred, 1-y_pred])\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15309a-e53f-476b-bd70-acc567407175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = Crossentropy()\n",
    "print(f\"Loss = {round(ce(y_true, y_pred_proba), 2)}, Derivative = {ce.derivative(y_true, y_pred_proba)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9bb36-8742-4c46-bf8b-cf83108f2fff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb443d-b218-4d04-9108-bce59294abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_multiclass, y_pred_multiclass_preprocessed = np.random.randint(0, 3, size=(5,)), np.random.uniform(0,1,(5, 3))\n",
    "y_pred_multiclass = y_pred_multiclass_preprocessed / y_pred_multiclass_preprocessed.sum(axis=1, keepdims=True)\n",
    "print(f\"{y_true_multiclass}\\n\\n{y_pred_multiclass}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a39cf-aa0d-4c0b-bf91-cd6f871dc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss = {round(ce(y_true_multiclass, y_pred_multiclass), 2)}, Derivative = {ce.derivative(y_true_multiclass, y_pred_multiclass), 2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50f2b4-75da-4612-aacf-cfffe6d7f47d",
   "metadata": {},
   "source": [
    "Rest assured, the return value is a numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013a013-18a7-43cf-9d0c-d740b36a03a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d43fdc-13ec-499e-9a50-881c2a141b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt, yp = np.random.randint(0,10,(4,)), np.random.randint(0,10,(4,))\n",
    "print(f\"{yt}\\n\\n{yp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b88c25-38c6-4546-8524-7516a0323478",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_fn = MSE()\n",
    "print(f\"Loss = {mse_loss_fn(yt, yp)}\\nderivative = \\n{mse_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64eb971-49c3-4113-97e9-fdf24875b32b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42f94-9aac-4843-992a-32b41fc26515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_loss_fn = MAE()\n",
    "print(f\"Loss = {mae_loss_fn(yt, yp)}\\nderivative = \\n{mae_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f63389-6edc-4423-b5a8-afbabf20b9ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ca8e-1805-423a-b525-2b2a650e28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss_fn = HuberLoss(delta=8)\n",
    "print(f\"Delta = {huber_loss_fn.delta}, Loss = {huber_loss_fn(yt, yp)}\\nderivative = \\n{huber_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93c7d-341c-47fe-9907-d772833d6f01",
   "metadata": {},
   "source": [
    "### Define Layer and Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9443c405-b87a-42df-b3fc-1b07aebf5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListHandler(logging.Handler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log = []\n",
    "\n",
    "    def emit(self, record):\n",
    "        self.log.append(self.format(record))\n",
    "\n",
    "class Layer():\n",
    "    \n",
    "    __valid_activations = {'sigmoid': Sigmoid, 'tanh': Tanh, 'relu': ReLU, 'leaky_relu': LeakyReLU, 'linear': Linear}\n",
    "    \n",
    "    def __init__(self, activation, in_dim, out_dim, learning_rate=0.01):\n",
    "        if activation.lower() not in list(Layer.__valid_activations.keys()):\n",
    "            raise Exception(f\"Valid activations are {Layer.__valid_activations}.\")\n",
    "        self.activation = Layer.__valid_activations[activation.lower()]()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "        self.batch_size = None\n",
    "\n",
    "        logger_name = f'MyClass_{id(self)}'  # Generate a unique name for the logger\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.log_handler = ListHandler()\n",
    "        self.log_handler.setLevel(logging.DEBUG)\n",
    "        self.logger.addHandler(self.log_handler)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - \\n%(message)s')\n",
    "        self.log_handler.setFormatter(formatter)\n",
    "\n",
    "    def forward_compute(self, X, compute_gradient=False, print_logs=False):\n",
    "        '''\n",
    "        compute_gradient: bool , is True if updates are to be performed. is False if only a prediction is to be made in a single forward pass,\n",
    "        '''\n",
    "        output_prime = X.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "\n",
    "        if compute_gradient:\n",
    "            # computing stuff for eventual backpropagation\n",
    "            self._activation_gradient = self.activation.vectorized_derivative(output_prime) # of order n x out_dim\n",
    "            self._input = X.copy()\n",
    "        if print_logs:\n",
    "            self.logger.info(f\"\\tFORWARD COMPUTE: Input to this layer = \\n\\t{X[:5]}\\n\")\n",
    "            self.logger.info(f\"\\tFORWARD COMPUTE: Weights for this layer = \\n\\t{self.weights}\\n\")\n",
    "            self.logger.info(f\"\\tFORWARD COMPUTE: Output from this layer = \\n\\t{output_val[:5]}\\n\")\n",
    "        \n",
    "        return output_val\n",
    "\n",
    "    def backprop_compute(self, prev_grad_multipliers, print_logs=False):\n",
    "        gradient_mat = 1\n",
    "        \n",
    "        # To Do: find a way of multiplying pre v_grad_multipliers with this layer's gradient multiplier matrix.\n",
    "        '''\n",
    "        Needs: \n",
    "         1. current-layer's activation gradient matrix(as a function of this layer's input, i.e. prev layers output)\n",
    "         2. current-layer's input matrix\n",
    "        '''\n",
    "        output_prime = self._input.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "        \n",
    "        activ_prev_layer_output_element_wise_product = np.multiply(prev_grad_multipliers, self._activation_gradient) # of order n x out_dim\n",
    "        weights_gradient = self._input.T.dot(activ_prev_layer_output_element_wise_product)\n",
    "        update_qty = self.learning_rate * weights_gradient\n",
    "        old_weights = self.weights.copy()\n",
    "        self.weights = self.weights - update_qty\n",
    "\n",
    "        if print_logs:\n",
    "            self.logger.info(f\"\\tBACKPROP: Weights before update of this layer = \\n\\t{old_weights}\\n\")\n",
    "            self.logger.info(f\"\\tBACKPROP: Weights-gradient for this layer = \\n\\t{weights_gradient}\\n\")\n",
    "            self.logger.info(f\"\\tBACKPROP: Weight-update quantity for this layer = \\n\\t{update_qty}\\n\")\n",
    "            self.logger.info(f\"\\tBACKPROP: Weights post update for this layer = \\n\\t{self.weights}\\n\")\n",
    "        \n",
    "\n",
    "        send_mat_to_prev_layer = activ_prev_layer_output_element_wise_product.dot(self.weights.T) # of order n x in_dim, \n",
    "                                                                      # which is basically n x out_dim for the previous layer.\n",
    "        return send_mat_to_prev_layer\n",
    "\n",
    "    def print_logs(self):\n",
    "        for log_record in self.log_handler.log:\n",
    "            print(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0eb109d2-fe53-409a-8c4c-42da00d6e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSequential():\n",
    "    __valid_loss_functions = {\n",
    "        'crossentropy': Crossentropy, \n",
    "        'mse': MSE,\n",
    "        'mae': MAE,\n",
    "        'huber': HuberLoss\n",
    "    }\n",
    "\n",
    "    __valid_gd_types = ['batch', 'mini_batch', 'stochastic']\n",
    "\n",
    "    # def __init__(self, layers_arr: List, metrics_to_track, early_stopping=False):\n",
    "    def __init__(self, layers_arr: List[Layer]):\n",
    "        self._layers = layers_arr\n",
    "        self.loss_arr = []\n",
    "\n",
    "    def compile(self, loss_function, n_iter = 100):\n",
    "        if loss_function.lower() not in ModelSequential.__valid_loss_functions:\n",
    "            raise Exception(f\"Loss functions should be any of {ModelSequential.__valid_loss_functions.keys()}.\")\n",
    "        \n",
    "        self.loss_function = ModelSequential.__valid_loss_functions[loss_function.lower()]()\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def reweight_layers(self):\n",
    "        n_layers = len(self._layers)\n",
    "        for i in range(n_layers):\n",
    "            in_dim, out_dim = self._layers[i].in_dim, self._layers[i].out_dim\n",
    "            self._layers[i].weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "            print(f\"Weights for layer {i} set to \\n{self._layers[i].weights}\\n\\n\")\n",
    "\n",
    "    def batch_fit(self, X, y, print_logs=False):\n",
    "        '''\n",
    "        for a given samples of X(features),y(target) , fit the model for n_iter iterations.\n",
    "        X: features, all numeric columns, n x m matrix\n",
    "        y: target variable, all numeric columns, n x c matrix , c: no. of classes/no. of numerical variables.\n",
    "        '''\n",
    "\n",
    "        self.loss_arr.append(self.loss_function(y_true=y, y_pred=self.predict(X)))        \n",
    "        \n",
    "        n_layers = len(self._layers)\n",
    "        input_for_next_layer = X.copy()\n",
    "\n",
    "        # forward compute\n",
    "        for i in range(n_layers):\n",
    "            try:\n",
    "                input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True)\n",
    "            except ValueError as ve:\n",
    "                raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "            except RuntimeWarning as rw:\n",
    "                raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "        input_for_next_layer = self.loss_function.derivative(y_true=y, y_pred=y_pred)\n",
    "\n",
    "    \n",
    "        # backprop\n",
    "        for i in range(n_layers-1, -1, -1):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"error\")\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].backprop_compute(input_for_next_layer, print_logs)\n",
    "                except ValueError as ve:\n",
    "                    return Exception(f\"Backpropagation failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Backprop failed at layer {i} with the following exception:\\n {rw}\")\n",
    "        \n",
    "        self.loss_arr.append(self.loss_function(y_true=y, y_pred=self.predict(X)))\n",
    "    \n",
    "    def fit(self, X, y, gd_type='mini_batch', batch_size=None):\n",
    "        if gd_type.lower() not in list(ModelSequential.__valid_gd_types):\n",
    "            raise Exception(f\"Valid Gradient descent types are {ModelSequential.__valid_gd_types}.\")\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        if gd_type.lower() == 'batch':\n",
    "            batch_size = n_samples\n",
    "        elif gd_type.lower() == 'mini_batch':\n",
    "            if type(batch_size) != 'int' or type(batch_size) != 'int64' or batch_size >= n_samples:\n",
    "                batch_size=64\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        print(f\"Using batch size as {batch_size}...\\n\\n\")\n",
    "        n_batches = floor(n_samples/batch_size)\n",
    "        \n",
    "        # iterate over all epochs\n",
    "        for iter_ in tqdm(range(self.n_iter), desc='Iterations...'):\n",
    "            # iterate over each batch_size sized batch\n",
    "            for batch_iter in tqdm(range(n_batches), desc='Batches...->'):\n",
    "                X_new, y_new = X[batch_iter * batch_size:(batch_iter + 1) * batch_size, :].copy(), y[batch_iter * batch_size:(batch_iter + 1) * batch_size].copy()\n",
    "                if iter_ == 0 and batch_iter == 0:\n",
    "                    self.batch_fit(X_new, y_new, print_logs=True)\n",
    "                else:\n",
    "                    self.batch_fit(X_new, y_new)\n",
    "\n",
    "    def print_layer_logs(self):\n",
    "        for i in range(n_layers):\n",
    "            print(f\"Handling layer {i}...\")\n",
    "            self._layers[i].print_logs()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_layers = len(self._layers)\n",
    "        input_for_next_layer = X.copy()\n",
    "\n",
    "        # forward compute\n",
    "        for i in range(n_layers):\n",
    "            with warnings.catch_warnings():\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "        return input_for_next_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b5658-359e-446d-98d0-255e327c8e5e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0f5952f-fd79-4b14-ac5e-8ae75b3bcf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0f42f9b1-2396-4737-b445-fe16dd166250",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, l3 = Layer('sigmoid', X.shape[1], 3, learning_rate=0.05), Layer('leaky_relu', 3, 2, learning_rate=0.05), Layer('linear', 2, 1, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d84f8b8b-bbef-48fa-954b-fa66783db8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights_list from the file using NumPy\n",
    "loaded_weights = np.load('model_weights.npy', allow_pickle=True)\n",
    "\n",
    "# Separate the loaded weights into individual arrays\n",
    "loaded_layer1_weights = loaded_weights[0][0].copy()\n",
    "loaded_layer2_weights = loaded_weights[1][0].copy()\n",
    "loaded_layer3_weights = loaded_weights[2][0].copy()\n",
    "\n",
    "# Now you have loaded the weights into separate NumPy arrays\n",
    "# loaded_layer1_weights, loaded_layer2_weights, loaded_layer3_weights\n",
    "l1.weights = loaded_layer1_weights\n",
    "l2.weights = loaded_layer2_weights\n",
    "l3.weights = loaded_layer3_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05258ab2-eb49-4fe7-a2eb-29712d6379b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65107906]\n",
      " [0.5028274 ]]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_layer3_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "650b97fe-407a-47ec-b1a1-41d6321fc004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3344916   0.68226814]\n",
      " [-0.31428093  0.6829982 ]\n",
      " [-1.0913304  -0.75725216]]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_layer2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "48764d6d-78d3-4409-8cad-b8be8994f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSequential([l1, l2, l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d70ef83-9928-4d0a-8367-744dee21e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss_function='huber', n_iter=10)\n",
    "model.compile(loss_function='mse', n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff69c889-7e7e-40f9-aa43-190b52711599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29079.742540893883\n"
     ]
    }
   ],
   "source": [
    "init_pred = model.predict(X)\n",
    "print(model.loss_function(y_true=y, y_pred=init_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f01b1bad-4fec-4c46-ba0b-783b183e4199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "[[ 0.38481784 -0.2873984   0.2569083 ]\n",
      " [ 0.63190603  0.0207603  -0.01701885]\n",
      " [-0.14716172 -0.3139873   0.13053173]\n",
      " [-0.5871992   0.58255076 -0.06661969]\n",
      " [-0.22049424 -0.00403428 -0.14160568]\n",
      " [-0.3763931  -0.22384077 -0.27747136]\n",
      " [-0.04296637 -0.07706636  0.31943142]\n",
      " [-0.02129191 -0.07783592 -0.5941539 ]\n",
      " [-0.17432415 -0.03620058  0.60362446]\n",
      " [ 0.40245163  0.31902552 -0.0473246 ]]\n",
      "\n",
      "\n",
      "Layer 1\n",
      "[[-0.3344916   0.68226814]\n",
      " [-0.31428093  0.6829982 ]\n",
      " [-1.0913304  -0.75725216]]\n",
      "\n",
      "\n",
      "Layer 2\n",
      "[[0.65107906]\n",
      " [0.5028274 ]]\n",
      "\n",
      "\n",
      "Using batch size as 64...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0072841644287109375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "Iterations...",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4f640d08134a1b809ba70c6077c055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00545501708984375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "Batches...->",
       "rate": null,
       "total": 6,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c502d10e21e4dbd91566d06316a4be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches...->:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Backprop failed at layer 0 with the following exception:\n invalid value encountered in multiply",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 60\u001b[0m, in \u001b[0;36mModelSequential.batch_fit\u001b[0;34m(self, X, y, print_logs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     input_for_next_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_for_next_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n",
      "Cell \u001b[0;32mIn[41], line 63\u001b[0m, in \u001b[0;36mLayer.backprop_compute\u001b[0;34m(self, prev_grad_multipliers, print_logs)\u001b[0m\n\u001b[1;32m     61\u001b[0m output_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(output_prime) \u001b[38;5;66;03m# of order n x out_dim\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m activ_prev_layer_output_element_wise_product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_grad_multipliers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activation_gradient\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# of order n x out_dim\u001b[39;00m\n\u001b[1;32m     64\u001b[0m weights_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(activ_prev_layer_output_element_wise_product)\n",
      "\u001b[0;31mRuntimeWarning\u001b[0m: invalid value encountered in multiply",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m_layers[i]\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 92\u001b[0m, in \u001b[0;36mModelSequential.fit\u001b[0;34m(self, X, y, gd_type, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_fit(X_new, y_new, print_logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_new\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 64\u001b[0m, in \u001b[0;36mModelSequential.batch_fit\u001b[0;34m(self, X, y, print_logs)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackpropagation failed at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the following exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m rw:\n\u001b[0;32m---> 64\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackprop failed at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the following exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_arr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(y_true\u001b[38;5;241m=\u001b[39my, y_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X)))\n",
      "\u001b[0;31mException\u001b[0m: Backprop failed at layer 0 with the following exception:\n invalid value encountered in multiply"
     ]
    }
   ],
   "source": [
    "# model.reweight_layers()\n",
    "\n",
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")\n",
    "\n",
    "\n",
    "model.fit(X, y, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01a090fd-b141-4d9a-9a07-26071208be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 09:59:58,709 - MyClass_6054062064 - INFO - \n",
      "\tBACKPROP: Weights before update of this layer = \n",
      "\t[[0.65107906]\n",
      " [0.5028274 ]]\n",
      "\n",
      "2023-11-29 09:59:58,709 - MyClass_6054062064 - INFO - \n",
      "\tBACKPROP: Weights-gradient for this layer = \n",
      "\t[[ 71.88931727]\n",
      " [-82.75701192]]\n",
      "\n",
      "2023-11-29 09:59:58,709 - MyClass_6054062064 - INFO - \n",
      "\tBACKPROP: Weight-update quantity for this layer = \n",
      "\t[[ 3.59446586]\n",
      " [-4.1378506 ]]\n",
      "\n",
      "2023-11-29 09:59:58,709 - MyClass_6054062064 - INFO - \n",
      "\tBACKPROP: Weights post update for this layer = \n",
      "\t[[-2.9433868]\n",
      " [ 4.640678 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model._layers[2].print_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85acccb6-c5fa-4c50-a9ba-85c4cee58e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 09:59:58,710 - MyClass_6054063552 - INFO - \n",
      "\tBACKPROP: Weights before update of this layer = \n",
      "\t[[-0.3344916   0.68226814]\n",
      " [-0.31428093  0.6829982 ]\n",
      " [-1.0913304  -0.75725216]]\n",
      "\n",
      "2023-11-29 09:59:58,710 - MyClass_6054063552 - INFO - \n",
      "\tBACKPROP: Weights-gradient for this layer = \n",
      "\t[[ 121.17662885 -636.84201588]\n",
      " [ 121.07777436 -636.32248752]\n",
      " [ 121.881573   -640.54683959]]\n",
      "\n",
      "2023-11-29 09:59:58,710 - MyClass_6054063552 - INFO - \n",
      "\tBACKPROP: Weight-update quantity for this layer = \n",
      "\t[[  6.05883144 -31.84210079]\n",
      " [  6.05388872 -31.81612438]\n",
      " [  6.09407865 -32.02734198]]\n",
      "\n",
      "2023-11-29 09:59:58,710 - MyClass_6054063552 - INFO - \n",
      "\tBACKPROP: Weights post update for this layer = \n",
      "\t[[-6.39332305 32.52436894]\n",
      " [-6.36816965 32.49912256]\n",
      " [-7.18540906 31.27008982]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model._layers[1].print_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a0efc5a5-4f26-4239-99cb-44299140d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 09:59:58,713 - MyClass_6051467120 - INFO - \n",
      "\tBACKPROP: Weights before update of this layer = \n",
      "\t[[ 0.38481784 -0.2873984   0.2569083 ]\n",
      " [ 0.63190603  0.0207603  -0.01701885]\n",
      " [-0.14716172 -0.3139873   0.13053173]\n",
      " [-0.5871992   0.58255076 -0.06661969]\n",
      " [-0.22049424 -0.00403428 -0.14160568]\n",
      " [-0.3763931  -0.22384077 -0.27747136]\n",
      " [-0.04296637 -0.07706636  0.31943142]\n",
      " [-0.02129191 -0.07783592 -0.5941539 ]\n",
      " [-0.17432415 -0.03620058  0.60362446]\n",
      " [ 0.40245163  0.31902552 -0.0473246 ]]\n",
      "\n",
      "2023-11-29 09:59:58,713 - MyClass_6051467120 - INFO - \n",
      "\tBACKPROP: Weights-gradient for this layer = \n",
      "\t[[ 72.71771589  72.6645262   70.46615276]\n",
      " [ 11.06609168  10.97005852  10.68479269]\n",
      " [-44.60487937 -44.66964115 -43.23590991]\n",
      " [ 29.57770543  29.43229677  28.66354577]\n",
      " [118.88668169 118.66370185 114.90403887]\n",
      " [161.59461344 161.37765908 156.10981706]\n",
      " [-25.56196236 -25.53104693 -24.49743414]\n",
      " [ 66.97099186  66.80690729  64.60558725]\n",
      " [-48.84420494 -48.94586891 -47.2241913 ]\n",
      " [114.0332815  113.92188138 110.29706832]]\n",
      "\n",
      "2023-11-29 09:59:58,713 - MyClass_6051467120 - INFO - \n",
      "\tBACKPROP: Weight-update quantity for this layer = \n",
      "\t[[ 3.63588579  3.63322631  3.52330764]\n",
      " [ 0.55330458  0.54850293  0.53423963]\n",
      " [-2.23024397 -2.23348206 -2.1617955 ]\n",
      " [ 1.47888527  1.47161484  1.43317729]\n",
      " [ 5.94433408  5.93318509  5.74520194]\n",
      " [ 8.07973067  8.06888295  7.80549085]\n",
      " [-1.27809812 -1.27655235 -1.22487171]\n",
      " [ 3.34854959  3.34034536  3.23027936]\n",
      " [-2.44221025 -2.44729345 -2.36120956]\n",
      " [ 5.70166408  5.69609407  5.51485342]]\n",
      "\n",
      "2023-11-29 09:59:58,714 - MyClass_6051467120 - INFO - \n",
      "\tBACKPROP: Weights post update for this layer = \n",
      "\t[[-3.25106796 -3.92062471 -3.26639934]\n",
      " [ 0.07860145 -0.52774263 -0.55125849]\n",
      " [ 2.08308225  1.91949474  2.29232722]\n",
      " [-2.06608448 -0.88906407 -1.49979698]\n",
      " [-6.16482832 -5.93721937 -5.88680762]\n",
      " [-8.45612378 -8.29272373 -8.08296222]\n",
      " [ 1.23513175  1.19948598  1.54430313]\n",
      " [-3.3698415  -3.41818128 -3.82443324]\n",
      " [ 2.26788609  2.41109286  2.96483403]\n",
      " [-5.29921244 -5.37706855 -5.56217801]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model._layers[0].print_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2861423-e675-497b-a82c-f7616166367f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "model.loss_function(y_true=y, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e39e7-4935-457e-9bf0-c1e5e866329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda968d8-5ae8-4d0d-af75-2599fc7af9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e42df-a66f-408b-b865-25240a123d34",
   "metadata": {},
   "source": [
    "### Print all layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851fd04-e1aa-4622-8351-18e40e8a244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0143f7-f2a9-49c5-9933-a2156aece7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86345908-fcbf-4e19-8dd3-c58e115cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7893e-4064-401c-94c2-b4f4017a73b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
