{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce045c90-3774-45cd-9961-af37b9ef754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "from typing import List\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39db43-08af-48e9-8928-35fbfdd8eba4",
   "metadata": {},
   "source": [
    "### Define signature of a generic activation function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbacb7f1-367d-4696-b991-02a1ff3f194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class ActivationFunction(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def vectorized_function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def derivative(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def __call__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae26c33-fffa-419a-8b69-49ba10116019",
   "metadata": {},
   "source": [
    "### Define all activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b35986c-5a36-4535-8a90-c8e210ebf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -700, 700)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.vectorized_function(x)*(1-self.vectorized_function(x))\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        x = np.clip(x, -100, 100)\n",
    "        # print(x)\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return (expr1-expr2)/(expr1+expr2)\n",
    "        \n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def derivative(self, x):\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return 4/((expr1+expr2)**2)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return self.derivative(x)\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0,x) == 0:\n",
    "            return 0\n",
    "        return 1\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.vectorized_function(x)\n",
    "    def function(self, x):\n",
    "        return max(0.01*x, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0.01*x, x) == x:\n",
    "            return 1\n",
    "        return 0.01\n",
    "    def vectorized_function(self, x):\n",
    "        return np.vectorize(self.function)(x)\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.vectorize(self.derivative)(x)\n",
    "\n",
    "class Linear:\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    def vectorized_derivative(self, x):\n",
    "        return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68a466-0b6c-418a-99f9-7c1e8ae56e08",
   "metadata": {},
   "source": [
    "### Import existing loss functions and code new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f45c19-05de-465b-b4b9-fedabcd235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberLoss(y_true, y_pred, delta=10):\n",
    "    err = y_true - y_pred\n",
    "    n_samples = y_true.shape[0]\n",
    "    abs_err = np.abs(err)\n",
    "    delta_sq = 0.5*(delta ** 2)\n",
    "    huber_loss_vectorized = np.vectorize(lambda x: (x**2)*0.5 if x <= delta else delta*x - delta)\n",
    "    huber_loss_vec = huber_loss_vectorized(abs_err)\n",
    "    return np.sum(huber_loss_vec)/n_samples\n",
    "    # return np.sum(huber_loss_vectorized(abs_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8ffaaed-5437-49a6-9ea3-29cf1072ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss\n",
    "\n",
    "class Crossentropy:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return log_loss(y_true, y_pred, labels = np.arange(y_pred.shape[1]))\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        # return an n x 1 matrix\n",
    "        epsilon = 1e-6\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_pred_capped = np.clip(y_pred, epsilon, 1-epsilon) # cap predicted probabilities to avoid floating point issues after taking reciprocal\n",
    "        y_pred_inv = 1/y_pred_capped # 1/y_hat\n",
    "        n_classes = y_pred.shape[1]\n",
    "        y_true_proba = np.eye(n_classes)[y_true] # for ease of computing the derivative, basically a one-hot encoding\n",
    "        derivative_loss_arr = -np.multiply(y_true_proba, y_pred_inv)# -[y/y_hat, (1-y)/(1-y_hat)]\n",
    "        derivative_loss_arr = np.sum(derivative_loss_arr, axis=1) # sum up, i.e., y/y_hat + (1-y)/(1-y_hat)  \n",
    "        return derivative_loss_arr/n_samples\n",
    "\n",
    "class MSE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return (-2*(y_true_reshaped - y_pred))/n_samples\n",
    "\n",
    "class MAE:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        return np.where(y_true > y_pred, 1, -1)/n_samples\n",
    "\n",
    "class HuberLoss:\n",
    "    def __init__(self, delta=10):\n",
    "        self.delta=delta\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return self.function(y_true, y_pred)\n",
    "    def function(self, y_true, y_pred):\n",
    "        return huberLoss(y_true, y_pred, self.delta)\n",
    "    def derivative(self, y_true, y_pred):\n",
    "        n_samples = y_true.shape[0]\n",
    "        y_true_reshaped = y_true.copy()\n",
    "        y_true_reshaped = y_true_reshaped.reshape(-1, 1)\n",
    "        err = y_true_reshaped - y_pred\n",
    "        huber_loss_derivative_vectorized = np.vectorize(lambda x: x if np.abs(x) <= self.delta else -self.delta if x < 0 else self.delta)\n",
    "        return huber_loss_derivative_vectorized(err)/n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7b780-94bf-440b-960f-786e48f02727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8646a9d-c285-4312-9f7f-b51d062c1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = np.random.randint(0, 2, size=(3,)), np.random.uniform(0,1,(3,1))\n",
    "print(y_true,\"\\n\\n\", y_pred)\n",
    "y_pred_proba = np.hstack([y_pred, 1-y_pred])\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15309a-e53f-476b-bd70-acc567407175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = Crossentropy()\n",
    "print(f\"Loss = {round(ce(y_true, y_pred_proba), 2)}, Derivative = {ce.derivative(y_true, y_pred_proba)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9bb36-8742-4c46-bf8b-cf83108f2fff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb443d-b218-4d04-9108-bce59294abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_multiclass, y_pred_multiclass_preprocessed = np.random.randint(0, 3, size=(5,)), np.random.uniform(0,1,(5, 3))\n",
    "y_pred_multiclass = y_pred_multiclass_preprocessed / y_pred_multiclass_preprocessed.sum(axis=1, keepdims=True)\n",
    "print(f\"{y_true_multiclass}\\n\\n{y_pred_multiclass}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a39cf-aa0d-4c0b-bf91-cd6f871dc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss = {round(ce(y_true_multiclass, y_pred_multiclass), 2)}, Derivative = {ce.derivative(y_true_multiclass, y_pred_multiclass), 2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50f2b4-75da-4612-aacf-cfffe6d7f47d",
   "metadata": {},
   "source": [
    "Rest assured, the return value is a numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013a013-18a7-43cf-9d0c-d740b36a03a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d43fdc-13ec-499e-9a50-881c2a141b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt, yp = np.random.randint(0,10,(4,)), np.random.randint(0,10,(4,))\n",
    "print(f\"{yt}\\n\\n{yp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b88c25-38c6-4546-8524-7516a0323478",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_fn = MSE()\n",
    "print(f\"Loss = {mse_loss_fn(yt, yp)}\\nderivative = \\n{mse_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64eb971-49c3-4113-97e9-fdf24875b32b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e42f94-9aac-4843-992a-32b41fc26515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_loss_fn = MAE()\n",
    "print(f\"Loss = {mae_loss_fn(yt, yp)}\\nderivative = \\n{mae_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f63389-6edc-4423-b5a8-afbabf20b9ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test on Regression using Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08ca8e-1805-423a-b525-2b2a650e28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss_fn = HuberLoss(delta=8)\n",
    "print(f\"Delta = {huber_loss_fn.delta}, Loss = {huber_loss_fn(yt, yp)}\\nderivative = \\n{huber_loss_fn.derivative(yt, yp)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93c7d-341c-47fe-9907-d772833d6f01",
   "metadata": {},
   "source": [
    "### Define Layer and Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9443c405-b87a-42df-b3fc-1b07aebf5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListHandler(logging.Handler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log = []\n",
    "\n",
    "    def emit(self, record):\n",
    "        self.log.append(self.format(record))\n",
    "\n",
    "class Layer():\n",
    "    \n",
    "    __valid_activations = {'sigmoid': Sigmoid, 'tanh': Tanh, 'relu': ReLU, 'leaky_relu': LeakyReLU, 'linear': Linear}\n",
    "    \n",
    "    def __init__(self, activation, in_dim, out_dim, learning_rate=0.01):\n",
    "        if activation.lower() not in list(Layer.__valid_activations.keys()):\n",
    "            raise Exception(f\"Valid activations are {Layer.__valid_activations}.\")\n",
    "        self.activation = Layer.__valid_activations[activation.lower()]()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "        self.batch_size = None\n",
    "\n",
    "        logger_name = f'MyClass_{id(self)}'  # Generate a unique name for the logger\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.log_handler = ListHandler()\n",
    "        self.log_handler.setLevel(logging.DEBUG)\n",
    "        self.logger.addHandler(self.log_handler)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - \\n%(message)s')\n",
    "        self.log_handler.setFormatter(formatter)\n",
    "\n",
    "    def forward_compute(self, X, compute_gradient=False, print_logs=False):\n",
    "        '''\n",
    "        compute_gradient: bool , is True if updates are to be performed. is False if only a prediction is to be made in a single forward pass,\n",
    "        '''\n",
    "        output_prime = X.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "\n",
    "        if compute_gradient:\n",
    "            # computing stuff for eventual backpropagation\n",
    "            self._activation_gradient = self.activation.vectorized_derivative(output_prime) # of order n x out_dim\n",
    "            self._input = X.copy()\n",
    "        if print_logs:\n",
    "            self.logger.info(f\"\\tFORWARD COMPUTE: Output from this layer = \\n\\t{output_val[:5]}\\n\")\n",
    "        \n",
    "        return output_val\n",
    "\n",
    "    def backprop_compute(self, prev_grad_multipliers):\n",
    "        gradient_mat = 1\n",
    "        \n",
    "        # To Do: find a way of multiplying pre v_grad_multipliers with this layer's gradient multiplier matrix.\n",
    "        '''\n",
    "        Needs: \n",
    "         1. current-layer's activation gradient matrix(as a function of this layer's input, i.e. prev layers output)\n",
    "         2. current-layer's input matrix\n",
    "        '''\n",
    "        output_prime = self._input.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "        \n",
    "        activ_prev_layer_output_element_wise_product = np.multiply(prev_grad_multipliers, self._activation_gradient) # of order n x out_dim\n",
    "        weights_gradient = self._input.T.dot(activ_prev_layer_output_element_wise_product)\n",
    "        \n",
    "        self.logger.info(f\"\\tInput to this layer = \\n\\t{self._input[:5]}\\n\")\n",
    "        # self.logger.info(f\"\\tOutput from this layer = \\n\\t{output_val[:5]}\\n\")\n",
    "        \n",
    "        self.weights -= self.learning_rate * weights_gradient\n",
    "        \n",
    "\n",
    "        send_mat_to_prev_layer = activ_prev_layer_output_element_wise_product.dot(self.weights.T) # of order n x in_dim, \n",
    "                                                                      # which is basically n x out_dim for the previous layer.\n",
    "        return send_mat_to_prev_layer\n",
    "\n",
    "    def print_logs(self):\n",
    "        for log_record in self.log_handler.log:\n",
    "            print(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0eb109d2-fe53-409a-8c4c-42da00d6e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSequential():\n",
    "    __valid_loss_functions = {\n",
    "        'crossentropy': Crossentropy, \n",
    "        'mse': MSE,\n",
    "        'mae': MAE,\n",
    "        'huber': HuberLoss\n",
    "    }\n",
    "\n",
    "    __valid_gd_types = ['batch', 'mini_batch', 'stochastic']\n",
    "\n",
    "    # def __init__(self, layers_arr: List, metrics_to_track, early_stopping=False):\n",
    "    def __init__(self, layers_arr: List[Layer]):\n",
    "        self._layers = layers_arr\n",
    "        self.loss_arr = []\n",
    "\n",
    "    def compile(self, loss_function, n_iter = 100):\n",
    "        if loss_function.lower() not in ModelSequential.__valid_loss_functions:\n",
    "            raise Exception(f\"Loss functions should be any of {ModelSequential.__valid_loss_functions.keys()}.\")\n",
    "        \n",
    "        self.loss_function = ModelSequential.__valid_loss_functions[loss_function.lower()]()\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def reweight_layers(self):\n",
    "        n_layers = len(self._layers)\n",
    "        for i in range(n_layers):\n",
    "            in_dim, out_dim = self._layers[i].in_dim, self._layers[i].out_dim\n",
    "            self._layers[i].weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "            print(f\"Weights for layer {i} set to \\n{self._layers[i].weights}\\n\\n\")\n",
    "\n",
    "    def batch_fit(self, X, y):\n",
    "        '''\n",
    "        for a given samples of X(features),y(target) , fit the model for n_iter iterations.\n",
    "        X: features, all numeric columns, n x m matrix\n",
    "        y: target variable, all numeric columns, n x c matrix , c: no. of classes/no. of numerical variables.\n",
    "        '''\n",
    "\n",
    "        self.loss_arr.append(self.loss_function(y_true=y, y_pred=self.predict(X)))\n",
    "        \n",
    "        pbar_iterations = tqdm(self.n_iter)\n",
    "        pbar_iterations.set_description(\"#Iterations: \")\n",
    "        \n",
    "        for iter_ in range(self.n_iter):\n",
    "            n_layers = len(self._layers)\n",
    "            input_for_next_layer = X.copy()\n",
    "\n",
    "\n",
    "            # forward compute\n",
    "            for i in range(n_layers):\n",
    "                try:\n",
    "                    if i == 1:\n",
    "                        input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True, print_logs=True)\n",
    "                    else:\n",
    "                        input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer, compute_gradient=True)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "\n",
    "            y_pred = self.predict(X)\n",
    "            input_for_next_layer = self.loss_function.derivative(y_true=y, y_pred=y_pred)\n",
    "\n",
    "        \n",
    "            # backprop\n",
    "            for i in range(n_layers-1, -1, -1):\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"error\")\n",
    "                    try:\n",
    "                        input_for_next_layer = self._layers[i].backprop_compute(input_for_next_layer)\n",
    "                    except ValueError as ve:\n",
    "                        return Exception(f\"Backpropagation failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                    except RuntimeWarning as rw:\n",
    "                        raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "            \n",
    "            self.loss_arr.append(self.loss_function(y_true=y, y_pred=self.predict(X)))\n",
    "\n",
    "            pbar_iterations.update(1)\n",
    "        pbar_iterations.close()\n",
    "    \n",
    "    def fit(self, X, y, gd_type='mini_batch', batch_size=None):\n",
    "        if gd_type.lower() not in list(ModelSequential.__valid_gd_types):\n",
    "            raise Exception(f\"Valid Gradient descent types are {ModelSequential.__valid_gd_types}.\")\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        if gd_type.lower() == 'batch':\n",
    "            batch_size = n_samples\n",
    "        elif gd_type.lower() == 'mini_batch':\n",
    "            if type(batch_size) != 'int' or type(batch_size) != 'int64' or batch_size >= n_samples:\n",
    "                batch_size=64\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        print(f\"Using batch size as {batch_size}...\\n\\n\")\n",
    "        n_batches = floor(n_samples/batch_size)\n",
    "\n",
    "        # iterate over each batch_size sized batch\n",
    "        for batch_iter in range(n_batches):            \n",
    "            X_new, y_new = X[batch_iter * batch_size:(batch_iter + 1) * batch_size, :].copy(), y[batch_iter * batch_size:(batch_iter + 1) * batch_size].copy()\n",
    "            self.batch_fit(X_new, y_new)\n",
    "\n",
    "    def print_layer_logs(self):\n",
    "        for i in range(n_layers):\n",
    "            print(f\"Handling layer {i}...\")\n",
    "            self._layers[i].print_logs()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_layers = len(self._layers)\n",
    "        input_for_next_layer = X.copy()\n",
    "\n",
    "        # forward compute\n",
    "        for i in range(n_layers):\n",
    "            with warnings.catch_warnings():\n",
    "                try:\n",
    "                    input_for_next_layer = self._layers[i].forward_compute(input_for_next_layer)\n",
    "                except ValueError as ve:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n{ve}\")\n",
    "                except RuntimeWarning as rw:\n",
    "                    raise Exception(f\"Forward compute failed at layer {i} with the following exception:\\n {rw}\")\n",
    "        return input_for_next_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b5658-359e-446d-98d0-255e327c8e5e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0f5952f-fd79-4b14-ac5e-8ae75b3bcf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_diabetes\n",
    "\n",
    "# X, y = load_diabetes(return_X_y=True)\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0f42f9b1-2396-4737-b445-fe16dd166250",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2, l3 = Layer('sigmoid', X.shape[1], 3, learning_rate=0.05), Layer('leaky_relu', 3, 2, learning_rate=0.05), Layer('linear', 2, 1, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d84f8b8b-bbef-48fa-954b-fa66783db8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights_list from the file using NumPy\n",
    "loaded_weights = np.load('model_weights.npy', allow_pickle=True)\n",
    "\n",
    "# Separate the loaded weights into individual arrays\n",
    "loaded_layer1_weights = loaded_weights[0][0]\n",
    "loaded_layer2_weights = loaded_weights[1][0]\n",
    "loaded_layer3_weights = loaded_weights[2][0]\n",
    "\n",
    "# Now you have loaded the weights into separate NumPy arrays\n",
    "# loaded_layer1_weights, loaded_layer2_weights, loaded_layer3_weights\n",
    "l1.weights = loaded_layer1_weights\n",
    "l2.weights = loaded_layer2_weights\n",
    "l3.weights = loaded_layer3_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "05258ab2-eb49-4fe7-a2eb-29712d6379b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65107906]\n",
      " [0.5028274 ]]\n"
     ]
    }
   ],
   "source": [
    "print(loaded_layer3_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "48764d6d-78d3-4409-8cad-b8be8994f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSequential([l1, l2, l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6d70ef83-9928-4d0a-8367-744dee21e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss_function='huber', n_iter=10)\n",
    "model.compile(loss_function='mse', n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ff69c889-7e7e-40f9-aa43-190b52711599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29029.82125825195\n"
     ]
    }
   ],
   "source": [
    "init_pred = model.predict(X)\n",
    "print(model.loss_function(y_true=y, y_pred=init_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f01b1bad-4fec-4c46-ba0b-783b183e4199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "[[ 0.38481784 -0.2873984   0.2569083 ]\n",
      " [ 0.63190603  0.0207603  -0.01701885]\n",
      " [-0.14716172 -0.3139873   0.13053173]\n",
      " [-0.5871992   0.58255076 -0.06661969]\n",
      " [-0.22049424 -0.00403428 -0.14160568]\n",
      " [-0.3763931  -0.22384077 -0.27747136]\n",
      " [-0.04296637 -0.07706636  0.31943142]\n",
      " [-0.02129191 -0.07783592 -0.5941539 ]\n",
      " [-0.17432415 -0.03620058  0.60362446]\n",
      " [ 0.40245163  0.31902552 -0.0473246 ]]\n",
      "\n",
      "\n",
      "Layer 1\n",
      "[[-0.3344916   0.68226814]\n",
      " [-0.31428093  0.6829982 ]\n",
      " [-1.0913304  -0.75725216]]\n",
      "\n",
      "\n",
      "Layer 2\n",
      "[[0.65107906]\n",
      " [0.5028274 ]]\n",
      "\n",
      "\n",
      "Using batch size as 64...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006041765213012695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 27,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c93ce15813c41df88c960539abf3fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Forward compute failed at layer 2 with the following exception:\n overflow encountered in subtract",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 68\u001b[0m, in \u001b[0;36mModelSequential.batch_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     input_for_next_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_for_next_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n",
      "Cell \u001b[0;32mIn[197], line 67\u001b[0m, in \u001b[0;36mLayer.backprop_compute\u001b[0;34m(self, prev_grad_multipliers)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# self.logger.info(f\"\\tOutput from this layer = \\n\\t{output_val[:5]}\\n\")\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m weights_gradient\n\u001b[1;32m     70\u001b[0m send_mat_to_prev_layer \u001b[38;5;241m=\u001b[39m activ_prev_layer_output_element_wise_product\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;66;03m# of order n x in_dim, \u001b[39;00m\n",
      "\u001b[0;31mRuntimeWarning\u001b[0m: overflow encountered in subtract",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m_layers[i]\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[198], line 98\u001b[0m, in \u001b[0;36mModelSequential.fit\u001b[0;34m(self, X, y, gd_type, batch_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):            \n\u001b[1;32m     97\u001b[0m     X_new, y_new \u001b[38;5;241m=\u001b[39m X[batch_iter \u001b[38;5;241m*\u001b[39m batch_size:(batch_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size, :]\u001b[38;5;241m.\u001b[39mcopy(), y[batch_iter \u001b[38;5;241m*\u001b[39m batch_size:(batch_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_new\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[198], line 72\u001b[0m, in \u001b[0;36mModelSequential.batch_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackpropagation failed at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the following exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m rw:\n\u001b[0;32m---> 72\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward compute failed at layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the following exception:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_arr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(y_true\u001b[38;5;241m=\u001b[39my, y_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X)))\n\u001b[1;32m     76\u001b[0m pbar_iterations\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Forward compute failed at layer 2 with the following exception:\n overflow encountered in subtract"
     ]
    }
   ],
   "source": [
    "# model.reweight_layers()\n",
    "\n",
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")\n",
    "\n",
    "\n",
    "model.fit(X, y, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "01a090fd-b141-4d9a-9a07-26071208be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 02:31:40,924 - MyClass_5715041056 - INFO - \n",
      "\tFORWARD COMPUTE: Output from this layer = \n",
      "\t[[-0.00881138  0.30266174]\n",
      " [-0.00869384  0.29330342]\n",
      " [-0.00883482  0.3031546 ]\n",
      " [-0.00852007  0.30267831]\n",
      " [-0.00859889  0.29966052]]\n",
      "\n",
      "2023-11-27 02:31:40,927 - MyClass_5715041056 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[0.50973872 0.49716206 0.50799134]\n",
      " [0.49390647 0.49379042 0.50304404]\n",
      " [0.5187596  0.49035977 0.50933283]\n",
      " [0.48544146 0.49940304 0.48809936]\n",
      " [0.48653979 0.50100949 0.49452305]]\n",
      "\n",
      "2023-11-27 02:31:40,940 - MyClass_5715041056 - INFO - \n",
      "\tFORWARD COMPUTE: Output from this layer = \n",
      "\t[[-1.04327706e-02  6.14867711e+01]\n",
      " [-1.08765252e-02  6.40228321e+01]\n",
      " [-9.92640710e-03  5.84480870e+01]\n",
      " [-8.17846153e-03  4.83310876e+01]\n",
      " [-7.86080376e-03  4.62850753e+01]]\n",
      "\n",
      "2023-11-27 02:31:40,942 - MyClass_5715041056 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[0.64604683 0.63444812 0.63942382]\n",
      " [0.6660295  0.66547955 0.66767432]\n",
      " [0.6217095  0.59455525 0.60880264]\n",
      " [0.49750106 0.51147867 0.50006544]\n",
      " [0.47418432 0.48853849 0.48258103]]\n",
      "\n",
      "2023-11-27 02:31:40,951 - MyClass_5715041056 - INFO - \n",
      "\tFORWARD COMPUTE: Output from this layer = \n",
      "\t[[-1.78000825e-306  2.12850461e-300]\n",
      " [-1.80534142e-002  2.15879761e+004]\n",
      " [-1.78000825e-306  2.12850461e-300]\n",
      " [-1.80534142e-002  2.15879761e+004]\n",
      " [-1.80534142e-002  2.15879761e+004]]\n",
      "\n",
      "2023-11-27 02:31:40,954 - MyClass_5715041056 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[9.85967654e-305 9.85967654e-305 9.85967654e-305]\n",
      " [1.00000000e+000 1.00000000e+000 1.00000000e+000]\n",
      " [9.85967654e-305 9.85967654e-305 9.85967654e-305]\n",
      " [1.00000000e+000 1.00000000e+000 1.00000000e+000]\n",
      " [1.00000000e+000 1.00000000e+000 1.00000000e+000]]\n",
      "\n",
      "2023-11-27 02:31:40,964 - MyClass_5715041056 - INFO - \n",
      "\tFORWARD COMPUTE: Output from this layer = \n",
      "\t[[-8.05125001e-298  9.62784463e-288]\n",
      " [-8.16583584e+006  9.76486864e+016]\n",
      " [-8.05125001e-298  9.62784463e-288]\n",
      " [-8.16583584e+006  9.76486864e+016]\n",
      " [-8.16583584e+006  9.76486864e+016]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model._layers[1].print_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "de1cad46-23c5-4c45-b5b4-b16512d5caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-27 02:29:52,596 - MyClass_5714031376 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[-0.00881138  0.30266174]\n",
      " [-0.00869384  0.29330342]\n",
      " [-0.00883482  0.3031546 ]\n",
      " [-0.00852007  0.30267831]\n",
      " [-0.00859889  0.29966052]]\n",
      "\n",
      "2023-11-27 02:29:52,611 - MyClass_5714031376 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[-1.04327706e-02  6.14867711e+01]\n",
      " [-1.08765252e-02  6.40228321e+01]\n",
      " [-9.92640710e-03  5.84480870e+01]\n",
      " [-8.17846153e-03  4.83310876e+01]\n",
      " [-7.86080376e-03  4.62850753e+01]]\n",
      "\n",
      "2023-11-27 02:29:52,622 - MyClass_5714031376 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[-1.78000825e-306  2.12850461e-300]\n",
      " [-1.80534142e-002  2.15879761e+004]\n",
      " [-1.78000825e-306  2.12850461e-300]\n",
      " [-1.80534142e-002  2.15879761e+004]\n",
      " [-1.80534142e-002  2.15879761e+004]]\n",
      "\n",
      "2023-11-27 02:29:52,633 - MyClass_5714031376 - INFO - \n",
      "\tInput to this layer = \n",
      "\t[[-8.05125001e-298  9.62784463e-288]\n",
      " [-8.16583584e+006  9.76486864e+016]\n",
      " [-8.05125001e-298  9.62784463e-288]\n",
      " [-8.16583584e+006  9.76486864e+016]\n",
      " [-8.16583584e+006  9.76486864e+016]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model._layers[2].print_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d2861423-e675-497b-a82c-f7616166367f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mMSE.__call__\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mMSE.function\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:101\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    104\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "model.loss_function(y_true=y, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df5e39e7-4935-457e-9bf0-c1e5e866329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "[[-385171.84 -379742.34 -393323.38]\n",
      " [-289175.16 -289028.84 -289466.56]\n",
      " [-361098.84 -361032.   -366622.97]\n",
      " [-458592.4  -464064.62 -465368.4 ]\n",
      " [-478988.56 -480969.22 -488474.25]\n",
      " [-496577.3  -498434.34 -503756.25]\n",
      " [ 367055.38  366053.84  362682.28]\n",
      " [-577394.7  -577423.8  -580373.44]\n",
      " [-462128.   -461964.94 -467120.38]\n",
      " [-438094.75 -436584.1  -439147.1 ]]\n",
      "\n",
      "\n",
      "Layer 1\n",
      "[[-2.7219453e+08  3.2549562e+16]\n",
      " [-2.7219453e+08  3.2549562e+16]\n",
      " [-2.7219453e+08  3.2549562e+16]]\n",
      "\n",
      "\n",
      "Layer 2\n",
      "[[1.4531762e+33]\n",
      " [         -inf]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eda968d8-5ae8-4d0d-af75-2599fc7af9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25925.177240833218, 21382.184231005536, 16928294.951077983, 1.0939799763713171e+19, 1.4035903790250388e+67, 1.1190986420038096e+269]\n"
     ]
    }
   ],
   "source": [
    "print(model.loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e42df-a66f-408b-b865-25240a123d34",
   "metadata": {},
   "source": [
    "### Print all layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851fd04-e1aa-4622-8351-18e40e8a244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(model._layers)\n",
    "for i in range(n_layers):\n",
    "    print(f\"Layer {i}\\n{model._layers[i].weights}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0143f7-f2a9-49c5-9933-a2156aece7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86345908-fcbf-4e19-8dd3-c58e115cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7893e-4064-401c-94c2-b4f4017a73b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
