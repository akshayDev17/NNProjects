{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce045c90-3774-45cd-9961-af37b9ef754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39db43-08af-48e9-8928-35fbfdd8eba4",
   "metadata": {},
   "source": [
    "### Define signature of a generic activation function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbacb7f1-367d-4696-b991-02a1ff3f194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class ActivationFunction(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def function(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def derivative(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def __call__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae26c33-fffa-419a-8b69-49ba10116019",
   "metadata": {},
   "source": [
    "### Define all activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b35986c-5a36-4535-8a90-c8e210ebf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "    def function(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def derivative(self, x):\n",
    "        return self.function(x)*(1-self.function(x))\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "    def function(self, x):\n",
    "        expression = np.exp(2*x)\n",
    "        return (expression-1)/(expression+1)\n",
    "    def derivative(self, x):\n",
    "        expr1, expr2 = np.exp(x), np.exp(-x)\n",
    "        return 4/((expr1+expr2)**2)\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "    def function(self, x):\n",
    "        return max(0, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0,x) == 0:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return self.function(x)\n",
    "    def function(self, x):\n",
    "        return max(0.01*x, x)\n",
    "    def derivative(self, x):\n",
    "        if max(0.01*x, x) == x:\n",
    "            return 1\n",
    "        return 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68a466-0b6c-418a-99f9-7c1e8ae56e08",
   "metadata": {},
   "source": [
    "### Import existing loss functions and code new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7f45c19-05de-465b-b4b9-fedabcd235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberLoss(y_true, y_pred, delta=10):\n",
    "    err = y_true - y_pred\n",
    "    abs_err = np.abs(err)\n",
    "    delta_sq = 0.5*(delta ** 2)\n",
    "    huber_loss_vectorized = np.vectorize(lambda x: (x**2)*0.5 if x <= delta else delta*x - delta)\n",
    "    huber_loss_vec = huber_loss_vectorized(abs_err)\n",
    "    return np.sum(huber_loss_vec)\n",
    "    # return np.sum(huber_loss_vectorized(abs_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93c7d-341c-47fe-9907-d772833d6f01",
   "metadata": {},
   "source": [
    "### Define Layer and Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9443c405-b87a-42df-b3fc-1b07aebf5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    __valid_activations = {'sigmoid': Sigmoid, 'tanh': Tanh, 'relu': ReLU, 'leaky_relu': LeakyReLU}\n",
    "    \n",
    "    def __init__(self, activation, in_dim, out_dim, learning_rate=0.01):\n",
    "        if activation.lower() not in list(Layer.__valid_activations.keys()):\n",
    "            raise Exception(f\"Valid activations are {Layer.__valid_activations}.\")\n",
    "        self.activation = Layer.__valid_activations[activation.lower()]()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.weights = np.random.uniform(-1,1, size=(in_dim, out_dim))\n",
    "\n",
    "    def forward_compute(self, X):\n",
    "        output_prime = X.dot(self.weights) # of order n x out_dim\n",
    "        output_val = self.activation(output_prime) # of order n x out_dim\n",
    "        \n",
    "        # computing stuff for eventual backpropagation\n",
    "        self._activation_gradient = self.activation.derivative(output_prime) # of order n x out_dim\n",
    "        self._input = X\n",
    "        \n",
    "        return output_val\n",
    "\n",
    "    def backprop_compute(self, prev_grad_multipliers):\n",
    "        gradient_mat = 1\n",
    "        \n",
    "        # To Do: find a way of multiplying pre v_grad_multipliers with this layer's gradient multiplier matrix.\n",
    "        '''\n",
    "        Needs: \n",
    "         1. current-layer's activation gradient matrix(as a function of this layer's input, i.e. prev layers output)\n",
    "         2. current-layer's input matrix\n",
    "        '''\n",
    "\n",
    "        activ_prev_layer_output_element_wise_product = np.multiply(prev_grad_multipliers, self._activation_gradient) # of order n x out_dim\n",
    "        weights_gradient = self._input.T.dot(activ_prev_layer_output_element_wise_product)\n",
    "        self.weights -= self.learning_rate * weights_gradient        \n",
    "\n",
    "        send_mat_to_prev_layer = activ_prev_layer_output_element_wise_product.dot(self.weights.T) # of order n x in_dim, \n",
    "                                                                      # which is basically n x out_dim for the previous layer.\n",
    "        return send_mat_to_prev_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58eb3897-91ed-4d52-a367-df964c2ed3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 9 5 3] \n",
      " [3 1 7 3] \n",
      "\n",
      " 30.5\n"
     ]
    }
   ],
   "source": [
    "y1 = np.random.randint(1,10,(4,))\n",
    "y2 = np.random.randint(1,10,(4,))\n",
    "print(y1, \"\\n\", y2, \"\\n\\n\", huberLoss(y1, y2, delta=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb109d2-fe53-409a-8c4c-42da00d6e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, mean_squared_error, mean_absolute_error, \n",
    "\n",
    "class ModelSequential():\n",
    "    __valid_loss_functions = {\n",
    "        'binary_crossentropy': BinaryCrossEntropy, 'categorical_crossentropy': CategoricalCrossEntropy, \n",
    "        'mse': MeanSquaredError\n",
    "    }\n",
    "\n",
    "    # def __init__(self, layers_arr: List, loss_function, metrics_to_track, early_stopping=False):\n",
    "    def __init__(self, layers_arr: List, loss_function):\n",
    "        self.__layers = layers_arr\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b5658-359e-446d-98d0-255e327c8e5e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d93386b8-4fbb-4913-be5c-56bc27c784c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240120 2400120 24000120 240000120\n",
      "\n",
      "80104 80104 8000104 240000120\n"
     ]
    }
   ],
   "source": [
    "def get_y(x):\n",
    "    return x[:,0] + (x[:,1]**2) + np.abs(x[:, 2])\n",
    "\n",
    "x_10K = np.random.randint(-100,100,size=(10000, 3))\n",
    "y_10K = get_y(x_10K)\n",
    "\n",
    "x_100K = np.random.randint(-100,100,size=(100000, 3))\n",
    "y_100K = get_y(x_10K)\n",
    "\n",
    "x_1M = np.random.randint(-100,100,size=(1000000, 3))\n",
    "y_1M = get_y(x_1M)\n",
    "\n",
    "x_10M = np.random.randint(-100,100,size=(10000000, 3))\n",
    "y_10M = get_y(x_10M)\n",
    "\n",
    "print(sys.getsizeof(x_10K), sys.getsizeof(x_100K), sys.getsizeof(x_1M), sys.getsizeof(x_10M))\n",
    "print()\n",
    "print(sys.getsizeof(y_10K), sys.getsizeof(y_100K), sys.getsizeof(y_1M), sys.getsizeof(x_10M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "690241d5-872e-4fc2-9454-be4ae6dcf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huber loss = 3372444578134.9873\n"
     ]
    }
   ],
   "source": [
    "l = Layer('Tanh', 3, 1, 0.01)\n",
    "y_10k_pred = l.forward_compute(x_10K)\n",
    "print(f\"Huber loss = {huberLoss(y_10K, y_10k_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42f9b1-2396-4737-b445-fe16dd166250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
